{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9745a99f",
   "metadata": {},
   "source": [
    "# üß† Entendimiento del problema de negocio\n",
    "\n",
    "## ‚úÖ Pasos generales en un problema de clasificaci√≥n\n",
    "\n",
    "1. **Entendimiento del problema** (selecci√≥n de la m√©trica m√°s adecuada)  \n",
    "2. **Obtenci√≥n de datos y primer contacto**  \n",
    "3. **Divisi√≥n en Train y Test**  \n",
    "4. **MiniEDA**: An√°lisis del target, an√°lisis bivariante, entendimiento de las features, selecci√≥n de las mismas (si es necesario)  \n",
    "5. **Preparaci√≥n del dataset de Train**: Conversi√≥n de categ√≥ricas, tratamiento de num√©ricas  \n",
    "6. **Selecci√≥n e instanciaci√≥n de modelos**. Baseline.  \n",
    "7. **Comparaci√≥n de modelos** (por validaci√≥n cruzada o tras optimizaci√≥n de hiperpar√°metros)  \n",
    "8. **Selecci√≥n de modelo final**: Optimizaci√≥n de hiperpar√°metros  \n",
    "9. **Equilibrado del train set**  \n",
    "10. **Evaluaci√≥n contra test**  \n",
    "11. **An√°lisis de errores y acciones futuras**  \n",
    "12. **Persistencia del modelo en disco**\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Gu√≠a pr√°ctica: ¬øQu√© m√©trica usar seg√∫n el problema?\n",
    "\n",
    "### 1. Clasificaci√≥n binaria\n",
    "\n",
    "| Negocio te dice... | Qu√© es importante | M√©trica principal | M√©tricas complementarias |\n",
    "|--------------------|-------------------|-------------------|---------------------------|\n",
    "| \"Evita tratar a quien no lo necesita\" | Minimizar FP | **Precision** | F1, matriz de confusi√≥n |\n",
    "| \"No podemos dejar sin tratar a un enfermo\" | Minimizar FN | **Recall** | F1, curva PR |\n",
    "| \"Queremos equilibrio entre FP y FN\" | Balance | **F1-score** | ROC-AUC, curva PR |\n",
    "| \"Queremos minimizar costes\" | Costes espec√≠ficos | **Custom loss** | Coste por predicci√≥n |\n",
    "\n",
    "> ‚ö†Ô∏è **Evitar accuracy si hay desbalance**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Clasificaci√≥n multiclase\n",
    "\n",
    "| Escenario | M√©tricas clave |\n",
    "|----------|----------------|\n",
    "| Clases equilibradas | Accuracy, macro F1 |\n",
    "| Clases desbalanceadas | Recall por clase, weighted F1 |\n",
    "| Coste desigual por clase | M√©tricas ponderadas (confusi√≥n por clase) |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Regresi√≥n\n",
    "\n",
    "| Negocio te dice... | Qu√© medir | M√©trica recomendada | Alternativas |\n",
    "|--------------------|-----------|----------------------|--------------|\n",
    "| \"Que no se equivoque mucho de media\" | Error medio | **MAE** | MSE |\n",
    "| \"Castiga m√°s los errores grandes\" | Errores extremos | **RMSE** | MAE |\n",
    "| \"Nos importa el porcentaje de error\" | Escala relativa | **MAPE** | MAE, RMSE |\n",
    "| \"Queremos saber si mejora la media\" | Explicaci√≥n global | **R¬≤** | RMSE, MAE |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Agrupamiento (Clustering)\n",
    "\n",
    "| Tipo de validaci√≥n | M√©trica recomendada |\n",
    "|--------------------|---------------------|\n",
    "| Interna (sin etiquetas reales) | Silhouette Score, Davies-Bouldin |\n",
    "| Externa (con etiquetas reales) | ARI, NMI, Homogeneity |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Series temporales\n",
    "\n",
    "| Requisito de negocio | M√©trica recomendada |\n",
    "|----------------------|---------------------|\n",
    "| Error absoluto | MAE |\n",
    "| Penalizar errores altos | RMSE |\n",
    "| Comparaci√≥n relativa | sMAPE, MASE |\n",
    "| Evaluaci√≥n visual | Gr√°fico real vs predicci√≥n |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Esquema r√°pido de decisi√≥n\n",
    "\n",
    "```markdown\n",
    "# ¬øQu√© tipo de problema tengo?\n",
    "- Binario ‚Üí Precision / Recall / F1\n",
    "- Multiclase ‚Üí Accuracy / macro F1\n",
    "- Regresi√≥n ‚Üí MAE, RMSE, MAPE\n",
    "- Clustering ‚Üí Silhouette, ARI\n",
    "- Series temporales ‚Üí MAE, RMSE, sMAPE\n",
    "\n",
    "# ¬øQu√© me pide negocio?\n",
    "- ¬øEvitar errores graves? ‚Üí Precision / Recall\n",
    "- ¬øEvitar errores grandes? ‚Üí RMSE\n",
    "- ¬øMedir error medio? ‚Üí MAE\n",
    "- ¬øMedir % de error? ‚Üí MAPE\n",
    "- ¬øExplicabilidad? ‚Üí R¬≤\n",
    "\n",
    "# ¬øQu√© hago para comparar?\n",
    "- M√©trica principal + matriz de confusi√≥n o gr√°fico\n",
    "- Validaci√≥n cruzada si hay pocos datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac771a",
   "metadata": {},
   "source": [
    "# üß† SPRINT 14: ML - Modelos No Supervisados II\n",
    "\n",
    "## üîª Reducci√≥n de Dimensionalidad\n",
    "\n",
    "### üåå Maldici√≥n de la Dimensionalidad\n",
    "\n",
    "- Cada instancia de un dataset es un vector en un espacio de `n` dimensiones.\n",
    "- A mayor n√∫mero de features, mayor complejidad y **dispersi√≥n** de los datos.\n",
    "- El n√∫mero de muestras necesarias para representar correctamente la informaci√≥n **crece exponencialmente** con el n√∫mero de dimensiones.\n",
    "\n",
    "#### Consecuencias:\n",
    "- Las instancias se alejan entre s√≠ ‚Üí m√°s dif√≠cil detectar patrones.\n",
    "- Riesgo de **overfitting**: nuevas observaciones se encuentran demasiado lejos de los datos originales.\n",
    "- Modelos menos robustos y generalizables.\n",
    "\n",
    "#### Soluci√≥n parcial:\n",
    "- Incrementar el conjunto de entrenamiento.\n",
    "- Aplicar **reducci√≥n de dimensionalidad** para simplificar el espacio de representaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivos de la Reducci√≥n de Dimensionalidad\n",
    "\n",
    "- Reducir la complejidad del modelo.\n",
    "- Mejorar la **capacidad de generalizaci√≥n**.\n",
    "- Disminuir el tiempo de procesamiento.\n",
    "- Eliminar ruido e irrelevancia.\n",
    "- Hacer m√°s robustos los modelos ante datos nuevos.\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Tipos de Reducci√≥n\n",
    "\n",
    "### 1. Selecci√≥n de Features (Feature Selection)\n",
    "\n",
    "- Se **mantienen las variables originales**, seleccionando solo las m√°s relevantes.\n",
    "- Puede hacerse con m√©todos **supervisados o no supervisados**.\n",
    "- Pros:\n",
    "  - Interpretabilidad total.\n",
    "- Contras:\n",
    "  - Se pierde la informaci√≥n de las relaciones entre features descartadas.\n",
    "  - Riesgo de no capturar combinaciones informativas.\n",
    "\n",
    "### 2. Transformaci√≥n de Features (Feature Extraction)\n",
    "\n",
    "- Se **crean nuevas variables** a partir de combinaciones de las originales.\n",
    "- Las nuevas variables pueden ser no interpretables directamente.\n",
    "- Proceso de **transformaci√≥n + selecci√≥n**.\n",
    "- Se busca condensar la informaci√≥n conservando la m√°xima varianza.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ T√©cnicas de Transformaci√≥n: Proyecci√≥n\n",
    "\n",
    "La proyecci√≥n transforma los datos a un espacio de menor dimensi√≥n preservando la mayor cantidad posible de informaci√≥n.\n",
    "\n",
    "### üìå PCA (An√°lisis de Componentes Principales)\n",
    "\n",
    "- Crea nuevas variables (componentes) como combinaciones lineales de las originales.\n",
    "- Se ordenan por varianza explicada.\n",
    "- Permite elegir cu√°ntas componentes mantener seg√∫n la varianza acumulada deseada.\n",
    "- Muy √∫til para visualizaci√≥n en 2D o 3D y como paso previo a clustering.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd2775",
   "metadata": {},
   "source": [
    "## üßÆ Introducci√≥n a PCA (Principal Component Analysis)\n",
    "\n",
    "### üìå ¬øQu√© es PCA?\n",
    "\n",
    "El An√°lisis de Componentes Principales (PCA) es una t√©cnica estad√≠stica de **proyecci√≥n** que:\n",
    "\n",
    "- Transforma un conjunto de variables originales correlacionadas en un conjunto nuevo de **componentes principales** no correlacionadas.\n",
    "- Busca **reducir la dimensionalidad** de un dataset manteniendo la **mayor cantidad posible de varianza** (informaci√≥n).\n",
    "- Es muy √∫til para simplificar espacios con muchas dimensiones, reduciendo ruido y facilitando el an√°lisis.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objetivo de PCA\n",
    "\n",
    "> Dado un dataset con `n` individuos y `p` variables (X‚ÇÅ, X‚ÇÇ, ..., X‚Çö), PCA intenta encontrar un n√∫mero `z < p` de **factores subyacentes** que expliquen casi la misma informaci√≥n que las `p` variables originales.\n",
    "\n",
    "Cada una de estas nuevas variables se llama **componente principal (CP)**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funciona?\n",
    "\n",
    "1. **Transformaci√≥n**:\n",
    "   A partir de `m` features originales (x‚ÇÅ, x‚ÇÇ, ..., x‚Çò), PCA genera **m componentes principales**, combinando las variables originales:\n",
    "\n",
    "CP1 = c‚ÇÅ‚ÇÄ¬∑z‚ÇÄ + c‚ÇÅ‚ÇÅ¬∑z‚ÇÅ + c‚ÇÅ‚ÇÇ¬∑z‚ÇÇ + ... + c‚ÇÅ‚Çò¬∑z‚Çò\n",
    "\n",
    "\n",
    "2. **Orden de importancia**:\n",
    "- Las componentes est√°n ordenadas por **varianza explicada**.\n",
    "- `CP1` es la m√°s informativa, `CP2` la segunda, y as√≠ sucesivamente.\n",
    "- Se seleccionan las `n` primeras componentes que explican un **porcentaje deseado** de la varianza original.\n",
    "\n",
    "3. **Reducci√≥n**:\n",
    "- Cada nueva observaci√≥n se representa por sus valores en las `n` componentes seleccionadas.\n",
    "- Es decir, las features pasan a ser los valores proyectados en el nuevo espacio.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä ¬øQu√© representa la \"informaci√≥n\"?\n",
    "\n",
    "En PCA, la **informaci√≥n** se asocia a la **varianza**:\n",
    "\n",
    "- Se intenta **maximizar la varianza** capturada por cada componente.\n",
    "- Se preservan tambi√©n las **covarianzas** entre variables, reflejando la estructura interna del dataset.\n",
    "\n",
    "> **Cuanta m√°s varianza conserva un componente, m√°s \"informativo\" es.**\n",
    "\n",
    "---\n",
    "\n",
    "### üìê ¬øQu√© hace PCA exactamente?\n",
    "\n",
    "- Identifica los **hiperplanos** del espacio de variables que **maximizan la varianza**.\n",
    "- **Proyecta** los datos sobre esos hiperplanos, minimizando la p√©rdida de informaci√≥n.\n",
    "- Reduce el n√∫mero de dimensiones manteniendo la estructura de los datos.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† ¬øPara qu√© se utiliza PCA?\n",
    "\n",
    "- üîç **Extracci√≥n de patrones**: permite detectar similitudes y diferencias en los datos.\n",
    "- ‚úÇÔ∏è **Reducci√≥n de dimensionalidad**: √∫til en datasets con cientos o miles de variables.\n",
    "- üìâ **Visualizaci√≥n**: transforma datos de muchas dimensiones a 2D o 3D para facilitar su representaci√≥n gr√°fica.\n",
    "- ‚öôÔ∏è **Preprocesamiento previo a clustering o modelos supervisados**: mejora eficiencia y reduce sobreajuste.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Ejemplo pr√°ctico en c√≥digo\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Escalado de los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicaci√≥n de PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Varianza explicada\n",
    "print(pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "‚úÖ Resumen: PCA es una t√©cnica de transformaci√≥n no supervisada que permite simplificar datasets complejos sin perder informaci√≥n esencial. Ideal para visualizar, reducir dimensionalidad y preparar datos para clustering o modelado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8b57c",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Algoritmo PCA\n",
    "\n",
    "### üß† Conceptos matem√°ticos clave\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä Matriz de Covarianza\n",
    "\n",
    "La **covarianza** mide en qu√© medida dos variables var√≠an conjuntamente respecto a sus medias:\n",
    "\n",
    "- Si `Cov(X, Y) > 0`: ambas variables tienden a aumentar juntas (relaci√≥n positiva).\n",
    "- Si `Cov(X, Y) < 0`: cuando una sube, la otra tiende a bajar (relaci√≥n negativa).\n",
    "- Si `Cov(X, Y) = 0`: no hay relaci√≥n lineal entre las variables.\n",
    "\n",
    "**F√≥rmula de la covarianza:**\n",
    "\n",
    "\\[\n",
    "\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})\n",
    "\\]\n",
    "\n",
    "- La **matriz de covarianza** recoge todas las covarianzas posibles entre pares de variables.\n",
    "- Es similar a la **matriz de correlaci√≥n**, pero sin escalar los valores. La correlaci√≥n est√° acotada entre -1 y 1, la covarianza no.\n",
    "\n",
    "Ejemplo para un dataset con dos features `b‚ÇÄ` y `b‚ÇÅ`:\n",
    "\n",
    "\\[\n",
    "\\text{CovMatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\text{Cov}(b_0, b_0) & \\text{Cov}(b_0, b_1) \\\\\n",
    "\\text{Cov}(b_1, b_0) & \\text{Cov}(b_1, b_1)\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Autovalores y Autovectores\n",
    "\n",
    "En √°lgebra lineal:\n",
    "\n",
    "- Un **autovector** de una matriz es un vector que no cambia de direcci√≥n al multiplicarse por dicha matriz.\n",
    "- El escalar asociado se llama **autovalor** (Œª).\n",
    "\n",
    "\\[\n",
    "A \\cdot v = \\lambda \\cdot v\n",
    "\\]\n",
    "\n",
    "Donde:\n",
    "- `A` es una matriz cuadrada.\n",
    "- `v` es un autovector (|v| = 1).\n",
    "- `Œª` es el autovalor (potencia explicativa).\n",
    "\n",
    "**Propiedades:**\n",
    "- La matriz debe ser cuadrada.\n",
    "- Hay tantos autovalores como dimensiones tenga la matriz.\n",
    "- Los autovectores son ortogonales entre s√≠.\n",
    "- Cada autovalor indica cu√°nta **varianza** (informaci√≥n) aporta el autovector asociado.\n",
    "\n",
    "En PCA:\n",
    "- Los **autovectores** de la matriz de covarianza se convierten en los **componentes principales**.\n",
    "- Los **autovalores** indican la **importancia relativa** de cada componente.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Pasos del algoritmo PCA\n",
    "\n",
    "1. **Estandarizar los datos**  \n",
    "   PCA es sensible a la escala ‚Üí imprescindible escalar con `StandardScaler` o similar.\n",
    "\n",
    "2. **Calcular la matriz de covarianza**  \n",
    "   Analiza c√≥mo var√≠an conjuntamente las variables del dataset.\n",
    "\n",
    "3. **Obtener autovalores y autovectores**  \n",
    "   Se descompone la matriz de covarianza para obtener las **direcciones de m√°xima varianza**.\n",
    "\n",
    "4. **Ordenar los componentes**  \n",
    "   Los autovectores se ordenan seg√∫n sus autovalores de mayor a menor ‚Üí obtenemos los **componentes principales**.\n",
    "\n",
    "5. **Construir el nuevo espacio**  \n",
    "   Se seleccionan los `k` primeros componentes principales seg√∫n la **varianza acumulada deseada**.\n",
    "\n",
    "6. **Transformar el dataset**  \n",
    "   Aplicamos los autovectores seleccionados al dataset original ‚Üí obtenemos el dataset reducido.\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ Elecci√≥n del n√∫mero de componentes\n",
    "\n",
    "- Seleccionamos cu√°ntos componentes mantener seg√∫n el porcentaje de **varianza explicada acumulada**.\n",
    "- Si nos quedamos con todos los componentes, simplemente transformamos el dataset sin p√©rdida de informaci√≥n.\n",
    "- **El objetivo es reducir**, no solo transformar, por lo que tiene sentido quedarse con los componentes m√°s explicativos.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Consideraciones importantes\n",
    "\n",
    "- **Escalado de variables**  \n",
    "  PCA maximiza la varianza ‚Üí hay que asegurarse de que todas las variables est√©n en la misma escala.\n",
    "\n",
    "- **Outliers**  \n",
    "  La varianza es muy sensible a valores at√≠picos ‚Üí estudiar y tratar los outliers antes de aplicar PCA.\n",
    "\n",
    "- **Varianza explicada**  \n",
    "  Permite saber cu√°nta informaci√≥n se pierde al reducir la dimensionalidad. Lo ideal es usar el menor n√∫mero de componentes que expliquen un alto porcentaje (por ejemplo, 90-95%).\n",
    "\n",
    "```python\n",
    "# Ejemplo en c√≥digo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=0.95)  # Mantener el 95% de la varianza\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(pca.explained_variance_ratio_)  # Varianza explicada por componente\n",
    "```\n",
    "‚úÖ Resumen:\n",
    "El algoritmo PCA transforma el espacio de datos buscando las direcciones que maximizan la varianza, y permite comprimir datasets muy complejos en un espacio reducido con m√≠nima p√©rdida de informaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cbc29",
   "metadata": {},
   "source": [
    "La obtenci√≥n de la PCA siguentes pasos:  \n",
    "1. Escalado de las variables  \n",
    "2. Instanciaci√≥n PCA   \n",
    "3. \"Entrenamiento\"  \n",
    "4. Mostrar valores PCA [No siempre es necesario] en funci√≥n de las features iniciales\n",
    "5. An√°lisis de la varianza Explicada: Selecci√≥n del n√∫mero de componentes necesarias [Depende de la aplicaci√≥n]  \n",
    "6. Generaci√≥n del dataset \"PCAdo\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
