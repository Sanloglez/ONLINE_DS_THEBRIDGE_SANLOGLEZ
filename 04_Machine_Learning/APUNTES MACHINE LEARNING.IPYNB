{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dac771a",
   "metadata": {},
   "source": [
    "# SPRINT 12: ML: MODELOS SUPERVISADOS III. OTROS MODELOS Y REPASO\n",
    "\n",
    "## K-NEAREST NEIGHBOURS (KNN) - Clasificaci√≥n y Regresi√≥n\n",
    "\n",
    "KNN es un algoritmo de aprendizaje supervisado basado en la proximidad entre vectores. No requiere entrenamiento tradicional: simplemente almacena los datos y compara distancias al hacer predicciones.\n",
    "\n",
    "üß† Fundamento\n",
    "- Clasificaci√≥n: Asigna una clase en base a la moda (mayor√≠a) de los vecinos m√°s cercanos.\n",
    "\n",
    "- Regresi√≥n: Predice un valor promedio de los vecinos m√°s cercanos.\n",
    "\n",
    "üîÅ Pasos para aplicar KNN (clasificaci√≥n o regresi√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab153551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar y preparar datos\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 2. Escalar variables (muy importante en KNN)\n",
    "# KNN es muy sensible a la escala, as√≠ que es necesario normalizar \n",
    "# o estandarizar.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 3. Importar e instanciar el modelo\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier  # o KNeighborsRegressor\n",
    "knn = KNeighborsClassifier(n_neighbors=K)  # elegir K seg√∫n an√°lisis\n",
    "\n",
    "# 4. Entrenar el modelo (aunque KNN \"entrena\" simplemente almacenando dataset)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 5. Realizar predicciones\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# 6. Evaluar el modelo\n",
    "# Clasificaci√≥n: accuracy, matriz de confusi√≥n, F1...\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Regresi√≥n: MAE, RMSE, R2\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a700da",
   "metadata": {},
   "source": [
    "‚öôÔ∏è Selecci√≥n del valor √≥ptimo de K\n",
    "- No hay un m√©todo exacto. Lo m√°s habitual es probar varios valores (ej: de 1 a 20) y elegir el que maximice el rendimiento.\n",
    "\n",
    "- Valores peque√±os ‚Üí muy sensibles al ruido.\n",
    "\n",
    "- Valores grandes ‚Üí modelos m√°s estables pero menos precisos en clases minoritarias.\n",
    "\n",
    "Ejemplo de b√∫squeda de K √≥ptimo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(f\"K={k} ‚Üí Accuracy: {knn.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467892f",
   "metadata": {},
   "source": [
    "üìé Notas clave\n",
    "- KNN es lento con muchos datos, porque calcula distancias con todos los puntos.\n",
    "\n",
    "- Puede usarse tanto para problemas de clasificaci√≥n como de regresi√≥n.\n",
    "\n",
    "- Requiere preprocesamiento cuidadoso (escalado de datos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd2775",
   "metadata": {},
   "source": [
    "## üìå Regresi√≥n Polin√≥mica (Polynomial Regression)\n",
    "\n",
    "Cuando la relaci√≥n entre variables no es lineal, una regresi√≥n lineal no es suficiente. La regresi√≥n polin√≥mica permite capturar relaciones no lineales mediante la transformaci√≥n de las variables originales en nuevas variables elevadas a potencias.\n",
    "\n",
    "üß† Fundamento\n",
    "Consiste en ampliar el espacio de variables incluyendo:\n",
    "\n",
    "Potencias de las features originales: \n",
    "ùë•\n",
    ",\n",
    "ùë•\n",
    "2\n",
    ",\n",
    "ùë•\n",
    "3\n",
    ",\n",
    "‚Ä¶\n",
    "x,x \n",
    "2\n",
    " ,x \n",
    "3\n",
    " ,‚Ä¶\n",
    "\n",
    "Combinaciones entre variables (si hay m√°s de una feature)\n",
    "\n",
    "Aunque el modelo sigue siendo lineal en los coeficientes, el comportamiento se vuelve no lineal respecto a las variables.\n",
    "\n",
    "üîÅ Pasos para aplicar regresi√≥n polin√≥mica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preparar los datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[[\"feature\"]].values\n",
    "y = df[\"target\"].values\n",
    "\n",
    "# 2. Transformar las variables con PolynomialFeatures\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=d)  # d es el grado del polinomio\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# 3. Entrenar el modelo de regresi√≥n lineal\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# 4. Predecir nuevos valores\n",
    "#Recordar: hay que transformar las features antes de predecir\n",
    "\n",
    "X_test_poly = poly.transform(X_test)\n",
    "y_pred = model.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4770c9",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Sobreajuste (Overfitting)\n",
    "Grados altos capturan muy bien el patr√≥n del entrenamiento, pero generalizan mal.\n",
    "\n",
    "Es recomendable:\n",
    "\n",
    "- Usar validaci√≥n cruzada para elegir el grado.\n",
    "\n",
    "- Visualizar el error en training vs test.\n",
    "\n",
    "- No pasar de grado 3-4 sin justificaci√≥n s√≥lida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220290bc",
   "metadata": {},
   "source": [
    "## üìå Support Vector Machine (SVM)\n",
    "\n",
    "SVM es un modelo supervisado de clasificaci√≥n (tambi√©n puede usarse en regresi√≥n). Su objetivo es encontrar el hiperplano que mejor separa las clases maximizando el margen entre los puntos de distintas clases.\n",
    "\n",
    "üß† Fundamento\n",
    "- Busca el separador lineal con mayor margen entre las clases.\n",
    "\n",
    "- Los puntos m√°s cercanos al hiperplano se llaman vectores de soporte, y determinan el modelo.\n",
    "\n",
    "- Si los datos no son separables linealmente, se recurre a kernels para transformar el espacio.\n",
    "\n",
    "üîÅ Pasos para aplicar un modelo SVM (clasificaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bffaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preparar los datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# 2. Escalado obligatorio\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 3. Instanciar y entrenar el modelo\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear', C=1)  # 'linear', 'poly', 'rbf'\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predecir y evaluar\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ecda6",
   "metadata": {},
   "source": [
    "üîÄ Par√°metros importantes\n",
    "- kernel: tipo de transformaci√≥n (ver m√°s abajo).\n",
    "\n",
    "- C: penalizaci√≥n por errores (cuanto mayor, menos margen permite ‚Üí modelo m√°s estricto).\n",
    "\n",
    "- gamma: en kernels RBF, controla cu√°nto influye un solo punto (bajo = amplio, alto = localizado).\n",
    "\n",
    "üß™ Kernels: separaci√≥n no lineal\n",
    "\n",
    "Cuando los datos no son linealmente separables, se usan kernels que transforman el espacio:\n",
    "| Kernel   | Descripci√≥n                              |\n",
    "| -------- | ---------------------------------------- |\n",
    "| `linear` | Hiperplano lineal                        |\n",
    "| `poly`   | Polin√≥mico (grado definido con `degree`) |\n",
    "| `rbf`    | Radial (transforma a dimensi√≥n infinita) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC(kernel='poly', degree=3)\n",
    "SVC(kernel='rbf', gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444c62c",
   "metadata": {},
   "source": [
    "## üìâ SVM en regresi√≥n (SVR)\n",
    "\n",
    "Tambi√©n puede usarse para regresi√≥n, con SVR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db278378",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Consideraciones clave\n",
    "- Muy sensible al escalado de los datos. Siempre escalar antes.\n",
    "\n",
    "- Sensibilidad a outliers: puede alterar el margen y los vectores de soporte.\n",
    "\n",
    "- No es recomendable para datasets grandes, ya que el entrenamiento puede ser costoso.\n",
    "\n",
    "- Hiperpar√°metro C controla el trade-off entre margen amplio y clasificaci√≥n perfecta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a5fe3",
   "metadata": {},
   "source": [
    "‚úÖ Gu√≠a paso a paso para un modelo supervisado con KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carga de datos\n",
    "df = pd.read_csv(\"ruta_al_csv\", sep=\";\")\n",
    "df.head()\n",
    "df.info()\n",
    "\n",
    "# 2. An√°lisis del target\n",
    "bt.pinta_distribucion_categoricas(df, [target], mostrar_valores=True, relativa=True)\n",
    "\n",
    "#üîπ Si est√° desbalanceado, ap√∫ntalo. Esto afectar√° al rendimiento de modelos \n",
    "# y puede requerir:\n",
    "# Usar m√©tricas robustas (recall, balanced accuracy)\n",
    "# Hacer balanceo (SMOTE, undersampling...) en casos graves\n",
    "\n",
    "# 3. Limpieza de columnas no √∫tiles (irrelevantes o con muchos nulos)\n",
    "df.drop(\"columna_con_nulos\", axis=1, inplace=True)\n",
    "\n",
    "# 4. Divisi√≥n Train/Test\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. An√°lisis de variables num√©ricas\n",
    "features_num = [\"var1\", \"var2\"]\n",
    "train_set[features_num].describe()\n",
    "train_set[features_num].hist()\n",
    "\n",
    "#üî∏ Revisa:\n",
    "# Rango entre variables ‚Üí escalado necesario para KNN\n",
    "# Distribuci√≥n sesgada ‚Üí aplica log si hay mucha asimetr√≠a\n",
    "\n",
    "# 6. An√°lisis de variables categ√≥ricas\n",
    "features_cat = [\"col_categorica\"]\n",
    "bt.pinta_distribucion_categoricas(train_set, features_cat, mostrar_valores=True, relativa=True)\n",
    "\n",
    "#‚úîÔ∏è Si es binaria, puedes codificarla como 0 y 1\n",
    "#‚úîÔ∏è Si tiene m√°s de dos categor√≠as, necesitar√°s one-hot encoding (para otros modelos)\n",
    "\n",
    "#7. Preprocesamiento de features\n",
    "\n",
    "# Categ√≥rica ‚Üí binaria\n",
    "train_set[\"col\"] = train_set[\"col\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "test_set[\"col\"] = test_set[\"col\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "\n",
    "# Num√©ricas ‚Üí escalado\n",
    "scaler = MinMaxScaler()\n",
    "train_set[features_num] = scaler.fit_transform(train_set[features_num])\n",
    "test_set[features_num] = scaler.transform(test_set[features_num])\n",
    "\n",
    "# 8. Separar X e y y entrenar KNN\n",
    "X_train = train_set[features_cat + features_num]\n",
    "y_train = train_set[target]\n",
    "X_test = test_set[features_cat + features_num]\n",
    "y_test = test_set[target]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 9. Evaluaci√≥n inicial\n",
    "print(classification_report(y_train, knn.predict(X_train)))\n",
    "print(classification_report(y_test, knn.predict(X_test)))\n",
    "\n",
    "# 10. Optimizar K manualmente\n",
    "metricas = []\n",
    "for k in range(1, 21):\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=\"balanced_accuracy\").mean()\n",
    "    metricas.append(score)\n",
    "\n",
    "best_k = np.argmax(metricas) + 1\n",
    "\n",
    "#‚úîÔ∏è Usa balanced_accuracy si el dataset est√° desbalanceado\n",
    "#‚úîÔ∏è Entrena y eval√∫a el modelo final con el mejor k\n",
    "\n",
    "# 11. B√∫squeda de hiperpar√°metros con GridSearch\n",
    "param_grid = {\n",
    "    \"n_neighbors\": range(1, 20),\n",
    "    \"weights\": [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring=\"balanced_accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(classification_report(y_test, grid.best_estimator_.predict(X_test)))\n",
    "\n",
    "# ‚úîÔ∏è weights=\"distance\" puede mejorar si hay ruido o vecinos poco representativos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7e18b",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Equilibrado de Clases (Class Imbalance)\n",
    "üìå ¬øQu√© es un dataset desbalanceado?\n",
    "Un dataset est√° desbalanceado cuando las clases del target no est√°n representadas de forma proporcional, es decir, una clase aparece con mucha m√°s frecuencia que otra.\n",
    "Ejemplo t√≠pico: 90% clase 'no', 10% clase 'yes'.\n",
    "\n",
    "‚ùó¬øPor qu√© es un problema?\n",
    "Los modelos pueden aprender a predecir siempre la clase mayoritaria y tener una alta accuracy pero bajo recall o F1-score para la clase minoritaria.\n",
    "\n",
    "üí° Consejo inicial:\n",
    "Antes de aplicar t√©cnicas, entiende bien el objetivo de negocio:\n",
    "\n",
    "- ¬øInteresa maximizar el recall de la clase minoritaria?\n",
    "\n",
    "- ¬øO reducir los falsos positivos?\n",
    "\n",
    "- ¬øQu√© cuesta y qu√© se gana con un TP, FP, FN, TN?\n",
    "\n",
    "### üß™ Estrategias para abordar el desequilibrio\n",
    "\n",
    "‚úÖ 0. No hacer nada (modelo base)\n",
    "Evaluar sin modificar el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab676ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad4acb",
   "metadata": {},
   "source": [
    "M√©tricas clave:\n",
    "\n",
    "- Precisi√≥n / recall de la clase minoritaria\n",
    "\n",
    "- Matriz de confusi√≥n: ConfusionMatrixDisplay.from_predictions(...)\n",
    "\n",
    "üîÅ 1. Over-sampling (sobremuestreo)\n",
    "\n",
    "üß¨ SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "Crea nuevas observaciones sint√©ticas de la clase minoritaria usando KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b122ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c985a5",
   "metadata": {},
   "source": [
    "Pros: mejora el recall sin perder datos\n",
    "Contras: riesgo de overfitting si se abusa\n",
    "\n",
    "üì¶ Otras alternativas de oversampling:\n",
    "\n",
    "- RandomOverSampler() (repite observaciones)\n",
    "\n",
    "- ADASYN() (parecido a SMOTE, pero adaptativo)\n",
    "\n",
    "üîª 2. Under-sampling (bajomuestreo)\n",
    "Reduce el n√∫mero de observaciones de la clase mayoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "may = X_train[y_train == \"no\"]\n",
    "min_ = X_train[y_train == \"yes\"]\n",
    "\n",
    "may_down = resample(may, replace=False, n_samples=len(min_), random_state=42)\n",
    "\n",
    "X_train_bal = pd.concat([may_down, min_])\n",
    "y_train_bal = pd.concat([y_train.loc[may_down.index], y_train.loc[min_.index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143cccd5",
   "metadata": {},
   "source": [
    "Pros: r√°pido y simple\n",
    "Contras: pierde datos valiosos ‚Üí riesgo de subentrenamiento\n",
    "\n",
    "‚öñÔ∏è 3. Ajuste de pesos (class_weight)\n",
    "\n",
    "Aplica un peso mayor a la clase minoritaria en la funci√≥n de p√©rdida del modelo. Recomendado si no quieres tocar el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80704cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45080eff",
   "metadata": {},
   "source": [
    "‚úîÔ∏è Muchos modelos de sklearn lo soportan (DecisionTree, SVM, RandomForest, etc.)\n",
    "\n",
    "üìä Comparativa de resultados\n",
    "| T√©cnica            | Precisi√≥n (YES) | Recall (YES) | Comentario                                         |\n",
    "| ------------------ | --------------- | ------------ | -------------------------------------------------- |\n",
    "| **Sin equilibrar** | 0.64            | 0.36         | Alta precisi√≥n clase NO, muy bajo recall clase YES |\n",
    "| **SMOTE**          | 0.51            | 0.61         | Mejora de recall, algo de precisi√≥n                |\n",
    "| **Under-sampling** | 0.39            | 0.85         | Mucho recall, baja precisi√≥n (dispara a todo)      |\n",
    "| **class\\_weight**  | 0.39            | 0.85         | Similar a under-sampling, pero sin eliminar datos  |\n",
    "\n",
    "### üß† Consejos finales para el equilibrio de clases\n",
    "\n",
    "‚ö†Ô∏è No hay una soluci√≥n universal ‚Üí prueba varias y eval√∫a\n",
    "\n",
    "üìâ Siempre eval√∫a con m√©tricas por clase: precisi√≥n, recall, F1\n",
    "\n",
    "üìä Usa validaci√≥n cruzada para testear robustez\n",
    "\n",
    "üéØ Ajusta el umbral de decisi√≥n (predict_proba) si no quieres reequilibrar el dataset\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2fff5",
   "metadata": {},
   "source": [
    "# üß† An√°lisis de Errores\n",
    "\n",
    "Analizar los errores es esencial para mejorar un modelo y adaptarlo a las prioridades reales del negocio. No basta con obtener un buen accuracy global: es necesario entender qu√© est√° fallando, c√≥mo y por qu√©.\n",
    "\n",
    "## ‚ö†Ô∏è Importancia del An√°lisis de Errores\n",
    "- Identifica clases con bajo rendimiento (recall, precisi√≥n...).\n",
    "\n",
    "- Detecta patrones sistem√°ticos de error.\n",
    "\n",
    "- Prioriza mejoras seg√∫n el impacto del fallo (no todas las clases pesan igual).\n",
    "\n",
    "- Te gu√≠a hacia una mejor interpretaci√≥n del modelo y ajustes m√°s efectivos.\n",
    "\n",
    "## üîç An√°lisis de errores en Clasificaci√≥n\n",
    "‚úÖ Proceso paso a paso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Entrena un modelo de clasificaci√≥n\n",
    "\n",
    "# 2. Eval√∫a con m√©tricas por clase\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# 3. Inspecciona la matriz de confusi√≥n normalizada (recall por fila)\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d52362",
   "metadata": {},
   "source": [
    "## üß≠ C√≥mo interpretar la matriz\n",
    "- Lectura por filas (con normalize=\"true\"):\n",
    "Cada fila representa una clase real ‚Üí el valor muestra a qu√© clase fue clasificada.\n",
    "\n",
    "- Las diagonales altas indican buen recall.\n",
    "\n",
    "- Las desviaciones fuera de la diagonal indican confusiones sistem√°ticas.\n",
    "\n",
    "üí° Ejemplo de an√°lisis cualitativo\n",
    "\n",
    "- Si clase 2 se clasifica mal y casi siempre como clase 4 o 5 ‚Üí hay un sesgo ascendente.\n",
    "\n",
    "- Si una clase con poco soporte tiene bajo recall, puedes:\n",
    "\n",
    "    -Reentrenar un modelo espec√≠fico para esa clase.\n",
    "\n",
    "    - Balancear las clases (oversampling).\n",
    "\n",
    "    - Aplicar un modelo de segundo nivel (\"modelo cascada\").\n",
    "\n",
    "\n",
    "## üìà Sugerencias para mejorar tras el an√°lisis\n",
    "1. Ingenier√≠a de variables\n",
    "Nuevas variables o transformar las existentes (escalado, codificaci√≥n ordinal...).\n",
    "\n",
    "2. Modelos especializados\n",
    "Para clases con errores sistem√°ticos o de alto impacto ‚Üí crear modelos dedicados.\n",
    "\n",
    "3. Reentrenar con class_weight\n",
    "Penalizar m√°s los errores en las clases minoritarias.\n",
    "\n",
    "4. Evaluar impacto real\n",
    "Discutir con negocio: ¬øcu√°l es el coste real de predecir mal cada clase?\n",
    "\n",
    "## üìâ An√°lisis de Errores en Regresi√≥n\n",
    "Cuando trabajamos con modelos de regresi√≥n, no podemos usar una matriz de confusi√≥n como en clasificaci√≥n, pero s√≠ podemos analizar los errores individuales (residuos), su distribuci√≥n y su relaci√≥n con las predicciones. Esto nos ayuda a identificar sesgos o zonas problem√°ticas.\n",
    "\n",
    "‚öôÔ∏è Preparaci√≥n del problema de regresi√≥n\n",
    "\n",
    "Se cambia el target a una variable num√©rica continua. En el ejemplo con el dataset diamonds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_regresion = \"price\"\n",
    "features_cat_reg = [\"color\", \"clarity\", \"cut\"]\n",
    "features_num_reg = [...]  # num√©ricas sin incluir \"price\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba28a9",
   "metadata": {},
   "source": [
    "Se aplica StandardScaler a las num√©ricas y luego se invierte el escalado del target para que vuelva a su escala real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"price\"] = scaler.inverse_transform(train_set[features_num])[..., price_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43aff5d",
   "metadata": {},
   "source": [
    "### üß™ M√©tricas de evaluaci√≥n en regresi√≥n\n",
    "| M√©trica                                  | Interpretaci√≥n                                                        |\n",
    "| ---------------------------------------- | --------------------------------------------------------------------- |\n",
    "| `MAE` (Error absoluto medio)             | Cu√°nto se equivoca el modelo de media (sin penalizar grandes errores) |\n",
    "| `MAPE` (Error porcentual medio)          | Cu√°nto se equivoca el modelo en proporci√≥n al valor real              |\n",
    "| `RMSE` (Ra√≠z del error cuadr√°tico medio) | Penaliza m√°s los errores grandes                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9765d4b",
   "metadata": {},
   "source": [
    "### üìä Visualizaciones esenciales\n",
    "1. Gr√°fico de reales vs predichos\n",
    "\n",
    "Permite detectar si el modelo funciona peor en ciertos rangos del target (por ejemplo, en precios altos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_vs_actual(y_real, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30cc936",
   "metadata": {},
   "source": [
    "Idealmente, los puntos deben estar alineados con la l√≠nea roja y = x.\n",
    "\n",
    "2. Distribuci√≥n de residuos\n",
    "\n",
    "Ayuda a ver si los errores tienen una distribuci√≥n sim√©trica o sesgada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos = y_test - y_pred\n",
    "sns.histplot(residuos, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48c9fc",
   "metadata": {},
   "source": [
    "Una distribuci√≥n normal de residuos suele indicar un modelo razonablemente ajustado.\n",
    "\n",
    "3. Residuos vs Predicciones\n",
    "\n",
    "Permite identificar zonas del rango de predicci√≥n donde el modelo sesga (por ejemplo, subestimando sistem√°ticamente):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred, residuos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801a23f",
   "metadata": {},
   "source": [
    "Un buen modelo deber√≠a mostrar los residuos dispersos alrededor del 0 sin patrones claros.\n",
    "\n",
    "### üß† ¬øQu√© hacer con la informaci√≥n del an√°lisis?\n",
    "\n",
    "En el ejemplo de diamonds, se observa que:\n",
    "\n",
    "El modelo funciona mejor en precios bajos.\n",
    "\n",
    "Para precios > 7500, hay mayor error y dispersi√≥n.\n",
    "\n",
    "El modelo parece sesgar m√°s en la \"cola larga\" de la distribuci√≥n.\n",
    "\n",
    "Esto puede indicar que:\n",
    "\n",
    "El modelo necesita mayor capacidad para manejar precios altos.\n",
    "\n",
    "Puede ser √∫til hacer un modelo espec√≠fico para ese segmento.\n",
    "\n",
    "Otra opci√≥n es binnear el target y convertirlo en clasificaci√≥n.\n",
    "\n",
    "### üí° Sugerencias de mejora\n",
    "\n",
    "1. Ingenier√≠a de caracter√≠sticas\n",
    "\n",
    "Transformar o crear nuevas features que ayuden al modelo.\n",
    "\n",
    "2. Modelos alternativos\n",
    "\n",
    "RandomForest, GradientBoosting, XGBoost...\n",
    "\n",
    "3. Ajuste de hiperpar√°metros\n",
    "\n",
    "Profundidad, n√∫mero de √°rboles, learning rate...\n",
    "\n",
    "4. Tratamiento de outliers\n",
    "\n",
    "Revisar los valores extremos que distorsionan la predicci√≥n.\n",
    "\n",
    "5. Segmentaci√≥n del modelo\n",
    "\n",
    "Crear un modelo exclusivo para valores altos o usar ensemble de modelos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
