{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9745a99f",
   "metadata": {},
   "source": [
    "# üß† Entendimiento del problema de negocio\n",
    "\n",
    "## ‚úÖ Pasos generales en un problema de clasificaci√≥n\n",
    "\n",
    "1. **Entendimiento del problema** (selecci√≥n de la m√©trica m√°s adecuada)  \n",
    "2. **Obtenci√≥n de datos y primer contacto**  \n",
    "3. **Divisi√≥n en Train y Test**  \n",
    "4. **MiniEDA**: An√°lisis del target, an√°lisis bivariante, entendimiento de las features, selecci√≥n de las mismas (si es necesario)  \n",
    "5. **Preparaci√≥n del dataset de Train**: Conversi√≥n de categ√≥ricas, tratamiento de num√©ricas  \n",
    "6. **Selecci√≥n e instanciaci√≥n de modelos**. Baseline.  \n",
    "7. **Comparaci√≥n de modelos** (por validaci√≥n cruzada o tras optimizaci√≥n de hiperpar√°metros)  \n",
    "8. **Selecci√≥n de modelo final**: Optimizaci√≥n de hiperpar√°metros  \n",
    "9. **Equilibrado del train set**  \n",
    "10. **Evaluaci√≥n contra test**  \n",
    "11. **An√°lisis de errores y acciones futuras**  \n",
    "12. **Persistencia del modelo en disco**\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Gu√≠a pr√°ctica: ¬øQu√© m√©trica usar seg√∫n el problema?\n",
    "\n",
    "### 1. Clasificaci√≥n binaria\n",
    "\n",
    "| Negocio te dice... | Qu√© es importante | M√©trica principal | M√©tricas complementarias |\n",
    "|--------------------|-------------------|-------------------|---------------------------|\n",
    "| \"Evita tratar a quien no lo necesita\" | Minimizar FP | **Precision** | F1, matriz de confusi√≥n |\n",
    "| \"No podemos dejar sin tratar a un enfermo\" | Minimizar FN | **Recall** | F1, curva PR |\n",
    "| \"Queremos equilibrio entre FP y FN\" | Balance | **F1-score** | ROC-AUC, curva PR |\n",
    "| \"Queremos minimizar costes\" | Costes espec√≠ficos | **Custom loss** | Coste por predicci√≥n |\n",
    "\n",
    "> ‚ö†Ô∏è **Evitar accuracy si hay desbalance**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Clasificaci√≥n multiclase\n",
    "\n",
    "| Escenario | M√©tricas clave |\n",
    "|----------|----------------|\n",
    "| Clases equilibradas | Accuracy, macro F1 |\n",
    "| Clases desbalanceadas | Recall por clase, weighted F1 |\n",
    "| Coste desigual por clase | M√©tricas ponderadas (confusi√≥n por clase) |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Regresi√≥n\n",
    "\n",
    "| Negocio te dice... | Qu√© medir | M√©trica recomendada | Alternativas |\n",
    "|--------------------|-----------|----------------------|--------------|\n",
    "| \"Que no se equivoque mucho de media\" | Error medio | **MAE** | MSE |\n",
    "| \"Castiga m√°s los errores grandes\" | Errores extremos | **RMSE** | MAE |\n",
    "| \"Nos importa el porcentaje de error\" | Escala relativa | **MAPE** | MAE, RMSE |\n",
    "| \"Queremos saber si mejora la media\" | Explicaci√≥n global | **R¬≤** | RMSE, MAE |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Agrupamiento (Clustering)\n",
    "\n",
    "| Tipo de validaci√≥n | M√©trica recomendada |\n",
    "|--------------------|---------------------|\n",
    "| Interna (sin etiquetas reales) | Silhouette Score, Davies-Bouldin |\n",
    "| Externa (con etiquetas reales) | ARI, NMI, Homogeneity |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Series temporales\n",
    "\n",
    "| Requisito de negocio | M√©trica recomendada |\n",
    "|----------------------|---------------------|\n",
    "| Error absoluto | MAE |\n",
    "| Penalizar errores altos | RMSE |\n",
    "| Comparaci√≥n relativa | sMAPE, MASE |\n",
    "| Evaluaci√≥n visual | Gr√°fico real vs predicci√≥n |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Esquema r√°pido de decisi√≥n\n",
    "\n",
    "```markdown\n",
    "# ¬øQu√© tipo de problema tengo?\n",
    "- Binario ‚Üí Precision / Recall / F1\n",
    "- Multiclase ‚Üí Accuracy / macro F1\n",
    "- Regresi√≥n ‚Üí MAE, RMSE, MAPE\n",
    "- Clustering ‚Üí Silhouette, ARI\n",
    "- Series temporales ‚Üí MAE, RMSE, sMAPE\n",
    "\n",
    "# ¬øQu√© me pide negocio?\n",
    "- ¬øEvitar errores graves? ‚Üí Precision / Recall\n",
    "- ¬øEvitar errores grandes? ‚Üí RMSE\n",
    "- ¬øMedir error medio? ‚Üí MAE\n",
    "- ¬øMedir % de error? ‚Üí MAPE\n",
    "- ¬øExplicabilidad? ‚Üí R¬≤\n",
    "\n",
    "# ¬øQu√© hago para comparar?\n",
    "- M√©trica principal + matriz de confusi√≥n o gr√°fico\n",
    "- Validaci√≥n cruzada si hay pocos datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac771a",
   "metadata": {},
   "source": [
    "# SPRINT 12: ML: MODELOS SUPERVISADOS III. OTROS MODELOS Y REPASO\n",
    "\n",
    "## K-NEAREST NEIGHBOURS (KNN) - Clasificaci√≥n y Regresi√≥n\n",
    "\n",
    "KNN es un algoritmo de aprendizaje supervisado basado en la proximidad entre vectores. No requiere entrenamiento tradicional: simplemente almacena los datos y compara distancias al hacer predicciones.\n",
    "\n",
    "üß† Fundamento\n",
    "- Clasificaci√≥n: Asigna una clase en base a la moda (mayor√≠a) de los vecinos m√°s cercanos.\n",
    "\n",
    "- Regresi√≥n: Predice un valor promedio de los vecinos m√°s cercanos.\n",
    "\n",
    "üîÅ Pasos para aplicar KNN (clasificaci√≥n o regresi√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab153551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar y preparar datos\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 2. Escalar variables (muy importante en KNN)\n",
    "# KNN es muy sensible a la escala, as√≠ que es necesario normalizar \n",
    "# o estandarizar.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 3. Importar e instanciar el modelo\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier  # o KNeighborsRegressor\n",
    "knn = KNeighborsClassifier(n_neighbors=K)  # elegir K seg√∫n an√°lisis\n",
    "\n",
    "# 4. Entrenar el modelo (aunque KNN \"entrena\" simplemente almacenando dataset)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 5. Realizar predicciones\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# 6. Evaluar el modelo\n",
    "# Clasificaci√≥n: accuracy, matriz de confusi√≥n, F1...\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Regresi√≥n: MAE, RMSE, R2\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a700da",
   "metadata": {},
   "source": [
    "‚öôÔ∏è Selecci√≥n del valor √≥ptimo de K\n",
    "- No hay un m√©todo exacto. Lo m√°s habitual es probar varios valores (ej: de 1 a 20) y elegir el que maximice el rendimiento.\n",
    "\n",
    "- Valores peque√±os ‚Üí muy sensibles al ruido.\n",
    "\n",
    "- Valores grandes ‚Üí modelos m√°s estables pero menos precisos en clases minoritarias.\n",
    "\n",
    "Ejemplo de b√∫squeda de K √≥ptimo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(f\"K={k} ‚Üí Accuracy: {knn.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467892f",
   "metadata": {},
   "source": [
    "üìé Notas clave\n",
    "- KNN es lento con muchos datos, porque calcula distancias con todos los puntos.\n",
    "\n",
    "- Puede usarse tanto para problemas de clasificaci√≥n como de regresi√≥n.\n",
    "\n",
    "- Requiere preprocesamiento cuidadoso (escalado de datos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd2775",
   "metadata": {},
   "source": [
    "## üìå Regresi√≥n Polin√≥mica (Polynomial Regression)\n",
    "\n",
    "Cuando la relaci√≥n entre variables no es lineal, una regresi√≥n lineal no es suficiente. La regresi√≥n polin√≥mica permite capturar relaciones no lineales mediante la transformaci√≥n de las variables originales en nuevas variables elevadas a potencias.\n",
    "\n",
    "üß† Fundamento\n",
    "Consiste en ampliar el espacio de variables incluyendo:\n",
    "\n",
    "Potencias de las features originales: \n",
    "ùë•\n",
    ",\n",
    "ùë•\n",
    "2\n",
    ",\n",
    "ùë•\n",
    "3\n",
    ",\n",
    "‚Ä¶\n",
    "x,x \n",
    "2\n",
    " ,x \n",
    "3\n",
    " ,‚Ä¶\n",
    "\n",
    "Combinaciones entre variables (si hay m√°s de una feature)\n",
    "\n",
    "Aunque el modelo sigue siendo lineal en los coeficientes, el comportamiento se vuelve no lineal respecto a las variables.\n",
    "\n",
    "üîÅ Pasos para aplicar regresi√≥n polin√≥mica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preparar los datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[[\"feature\"]].values\n",
    "y = df[\"target\"].values\n",
    "\n",
    "# 2. Transformar las variables con PolynomialFeatures\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=d)  # d es el grado del polinomio\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# 3. Entrenar el modelo de regresi√≥n lineal\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# 4. Predecir nuevos valores\n",
    "#Recordar: hay que transformar las features antes de predecir\n",
    "\n",
    "X_test_poly = poly.transform(X_test)\n",
    "y_pred = model.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4770c9",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Sobreajuste (Overfitting)\n",
    "Grados altos capturan muy bien el patr√≥n del entrenamiento, pero generalizan mal.\n",
    "\n",
    "Es recomendable:\n",
    "\n",
    "- Usar validaci√≥n cruzada para elegir el grado.\n",
    "\n",
    "- Visualizar el error en training vs test.\n",
    "\n",
    "- No pasar de grado 3-4 sin justificaci√≥n s√≥lida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220290bc",
   "metadata": {},
   "source": [
    "## üìå Support Vector Machine (SVM)\n",
    "\n",
    "SVM es un modelo supervisado de clasificaci√≥n (tambi√©n puede usarse en regresi√≥n). Su objetivo es encontrar el hiperplano que mejor separa las clases maximizando el margen entre los puntos de distintas clases.\n",
    "\n",
    "üß† Fundamento\n",
    "- Busca el separador lineal con mayor margen entre las clases.\n",
    "\n",
    "- Los puntos m√°s cercanos al hiperplano se llaman vectores de soporte, y determinan el modelo.\n",
    "\n",
    "- Si los datos no son separables linealmente, se recurre a kernels para transformar el espacio.\n",
    "\n",
    "üîÅ Pasos para aplicar un modelo SVM (clasificaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bffaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preparar los datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# 2. Escalado obligatorio\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 3. Instanciar y entrenar el modelo\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear', C=1)  # 'linear', 'poly', 'rbf'\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predecir y evaluar\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ecda6",
   "metadata": {},
   "source": [
    "üîÄ Par√°metros importantes\n",
    "- kernel: tipo de transformaci√≥n (ver m√°s abajo).\n",
    "\n",
    "- C: penalizaci√≥n por errores (cuanto mayor, menos margen permite ‚Üí modelo m√°s estricto).\n",
    "\n",
    "- gamma: en kernels RBF, controla cu√°nto influye un solo punto (bajo = amplio, alto = localizado).\n",
    "\n",
    "üß™ Kernels: separaci√≥n no lineal\n",
    "\n",
    "Cuando los datos no son linealmente separables, se usan kernels que transforman el espacio:\n",
    "| Kernel   | Descripci√≥n                              |\n",
    "| -------- | ---------------------------------------- |\n",
    "| `linear` | Hiperplano lineal                        |\n",
    "| `poly`   | Polin√≥mico (grado definido con `degree`) |\n",
    "| `rbf`    | Radial (transforma a dimensi√≥n infinita) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC(kernel='poly', degree=3)\n",
    "SVC(kernel='rbf', gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444c62c",
   "metadata": {},
   "source": [
    "## üìâ SVM en regresi√≥n (SVR)\n",
    "\n",
    "Tambi√©n puede usarse para regresi√≥n, con SVR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db278378",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Consideraciones clave\n",
    "- Muy sensible al escalado de los datos. Siempre escalar antes.\n",
    "\n",
    "- Sensibilidad a outliers: puede alterar el margen y los vectores de soporte.\n",
    "\n",
    "- No es recomendable para datasets grandes, ya que el entrenamiento puede ser costoso.\n",
    "\n",
    "- Hiperpar√°metro C controla el trade-off entre margen amplio y clasificaci√≥n perfecta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a5fe3",
   "metadata": {},
   "source": [
    "‚úÖ Gu√≠a paso a paso para un modelo supervisado con KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carga de datos\n",
    "df = pd.read_csv(\"ruta_al_csv\", sep=\";\")\n",
    "df.head()\n",
    "df.info()\n",
    "\n",
    "# 2. An√°lisis del target\n",
    "bt.pinta_distribucion_categoricas(df, [target], mostrar_valores=True, relativa=True)\n",
    "\n",
    "#üîπ Si est√° desbalanceado, ap√∫ntalo. Esto afectar√° al rendimiento de modelos \n",
    "# y puede requerir:\n",
    "# Usar m√©tricas robustas (recall, balanced accuracy)\n",
    "# Hacer balanceo (SMOTE, undersampling...) en casos graves\n",
    "\n",
    "# 3. Limpieza de columnas no √∫tiles (irrelevantes o con muchos nulos)\n",
    "df.drop(\"columna_con_nulos\", axis=1, inplace=True)\n",
    "\n",
    "# 4. Divisi√≥n Train/Test\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. An√°lisis de variables num√©ricas\n",
    "features_num = [\"var1\", \"var2\"]\n",
    "train_set[features_num].describe()\n",
    "train_set[features_num].hist()\n",
    "\n",
    "#üî∏ Revisa:\n",
    "# Rango entre variables ‚Üí escalado necesario para KNN\n",
    "# Distribuci√≥n sesgada ‚Üí aplica log si hay mucha asimetr√≠a\n",
    "\n",
    "# 6. An√°lisis de variables categ√≥ricas\n",
    "features_cat = [\"col_categorica\"]\n",
    "bt.pinta_distribucion_categoricas(train_set, features_cat, mostrar_valores=True, relativa=True)\n",
    "\n",
    "#‚úîÔ∏è Si es binaria, puedes codificarla como 0 y 1\n",
    "#‚úîÔ∏è Si tiene m√°s de dos categor√≠as, necesitar√°s one-hot encoding (para otros modelos)\n",
    "\n",
    "#7. Preprocesamiento de features\n",
    "\n",
    "# Categ√≥rica ‚Üí binaria\n",
    "train_set[\"col\"] = train_set[\"col\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "test_set[\"col\"] = test_set[\"col\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "\n",
    "# Num√©ricas ‚Üí escalado\n",
    "scaler = MinMaxScaler()\n",
    "train_set[features_num] = scaler.fit_transform(train_set[features_num])\n",
    "test_set[features_num] = scaler.transform(test_set[features_num])\n",
    "\n",
    "# 8. Separar X e y y entrenar KNN\n",
    "X_train = train_set[features_cat + features_num]\n",
    "y_train = train_set[target]\n",
    "X_test = test_set[features_cat + features_num]\n",
    "y_test = test_set[target]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 9. Evaluaci√≥n inicial\n",
    "print(classification_report(y_train, knn.predict(X_train)))\n",
    "print(classification_report(y_test, knn.predict(X_test)))\n",
    "\n",
    "# 10. Optimizar K manualmente\n",
    "metricas = []\n",
    "for k in range(1, 21):\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=\"balanced_accuracy\").mean()\n",
    "    metricas.append(score)\n",
    "\n",
    "best_k = np.argmax(metricas) + 1\n",
    "\n",
    "#‚úîÔ∏è Usa balanced_accuracy si el dataset est√° desbalanceado\n",
    "#‚úîÔ∏è Entrena y eval√∫a el modelo final con el mejor k\n",
    "\n",
    "# 11. B√∫squeda de hiperpar√°metros con GridSearch\n",
    "param_grid = {\n",
    "    \"n_neighbors\": range(1, 20),\n",
    "    \"weights\": [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring=\"balanced_accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(classification_report(y_test, grid.best_estimator_.predict(X_test)))\n",
    "\n",
    "# ‚úîÔ∏è weights=\"distance\" puede mejorar si hay ruido o vecinos poco representativos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7e18b",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Equilibrado de Clases (Class Imbalance)\n",
    "üìå ¬øQu√© es un dataset desbalanceado?\n",
    "Un dataset est√° desbalanceado cuando las clases del target no est√°n representadas de forma proporcional, es decir, una clase aparece con mucha m√°s frecuencia que otra.\n",
    "Ejemplo t√≠pico: 90% clase 'no', 10% clase 'yes'.\n",
    "\n",
    "‚ùó¬øPor qu√© es un problema?\n",
    "Los modelos pueden aprender a predecir siempre la clase mayoritaria y tener una alta accuracy pero bajo recall o F1-score para la clase minoritaria.\n",
    "\n",
    "üí° Consejo inicial:\n",
    "Antes de aplicar t√©cnicas, entiende bien el objetivo de negocio:\n",
    "\n",
    "- ¬øInteresa maximizar el recall de la clase minoritaria?\n",
    "\n",
    "- ¬øO reducir los falsos positivos?\n",
    "\n",
    "- ¬øQu√© cuesta y qu√© se gana con un TP, FP, FN, TN?\n",
    "\n",
    "### üß™ Estrategias para abordar el desequilibrio\n",
    "\n",
    "‚úÖ 0. No hacer nada (modelo base)\n",
    "Evaluar sin modificar el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab676ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad4acb",
   "metadata": {},
   "source": [
    "M√©tricas clave:\n",
    "\n",
    "- Precisi√≥n / recall de la clase minoritaria\n",
    "\n",
    "- Matriz de confusi√≥n: ConfusionMatrixDisplay.from_predictions(...)\n",
    "\n",
    "üîÅ 1. Over-sampling (sobremuestreo)\n",
    "\n",
    "üß¨ SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "Crea nuevas observaciones sint√©ticas de la clase minoritaria usando KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b122ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c985a5",
   "metadata": {},
   "source": [
    "Pros: mejora el recall sin perder datos\n",
    "Contras: riesgo de overfitting si se abusa\n",
    "\n",
    "üì¶ Otras alternativas de oversampling:\n",
    "\n",
    "- RandomOverSampler() (repite observaciones)\n",
    "\n",
    "- ADASYN() (parecido a SMOTE, pero adaptativo)\n",
    "\n",
    "üîª 2. Under-sampling (bajomuestreo)\n",
    "Reduce el n√∫mero de observaciones de la clase mayoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "may = X_train[y_train == \"no\"]\n",
    "min_ = X_train[y_train == \"yes\"]\n",
    "\n",
    "may_down = resample(may, replace=False, n_samples=len(min_), random_state=42)\n",
    "\n",
    "X_train_bal = pd.concat([may_down, min_])\n",
    "y_train_bal = pd.concat([y_train.loc[may_down.index], y_train.loc[min_.index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143cccd5",
   "metadata": {},
   "source": [
    "Pros: r√°pido y simple\n",
    "Contras: pierde datos valiosos ‚Üí riesgo de subentrenamiento\n",
    "\n",
    "‚öñÔ∏è 3. Ajuste de pesos (class_weight)\n",
    "\n",
    "Aplica un peso mayor a la clase minoritaria en la funci√≥n de p√©rdida del modelo. Recomendado si no quieres tocar el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80704cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45080eff",
   "metadata": {},
   "source": [
    "‚úîÔ∏è Muchos modelos de sklearn lo soportan (DecisionTree, SVM, RandomForest, etc.)\n",
    "\n",
    "üìä Comparativa de resultados\n",
    "| T√©cnica            | Precisi√≥n (YES) | Recall (YES) | Comentario                                         |\n",
    "| ------------------ | --------------- | ------------ | -------------------------------------------------- |\n",
    "| **Sin equilibrar** | 0.64            | 0.36         | Alta precisi√≥n clase NO, muy bajo recall clase YES |\n",
    "| **SMOTE**          | 0.51            | 0.61         | Mejora de recall, algo de precisi√≥n                |\n",
    "| **Under-sampling** | 0.39            | 0.85         | Mucho recall, baja precisi√≥n (dispara a todo)      |\n",
    "| **class\\_weight**  | 0.39            | 0.85         | Similar a under-sampling, pero sin eliminar datos  |\n",
    "\n",
    "### üß† Consejos finales para el equilibrio de clases\n",
    "\n",
    "‚ö†Ô∏è No hay una soluci√≥n universal ‚Üí prueba varias y eval√∫a\n",
    "\n",
    "üìâ Siempre eval√∫a con m√©tricas por clase: precisi√≥n, recall, F1\n",
    "\n",
    "üìä Usa validaci√≥n cruzada para testear robustez\n",
    "\n",
    "üéØ Ajusta el umbral de decisi√≥n (predict_proba) si no quieres reequilibrar el dataset\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2fff5",
   "metadata": {},
   "source": [
    "# üß† An√°lisis de Errores\n",
    "\n",
    "Analizar los errores es esencial para mejorar un modelo y adaptarlo a las prioridades reales del negocio. No basta con obtener un buen accuracy global: es necesario entender qu√© est√° fallando, c√≥mo y por qu√©.\n",
    "\n",
    "## ‚ö†Ô∏è Importancia del An√°lisis de Errores\n",
    "- Identifica clases con bajo rendimiento (recall, precisi√≥n...).\n",
    "\n",
    "- Detecta patrones sistem√°ticos de error.\n",
    "\n",
    "- Prioriza mejoras seg√∫n el impacto del fallo (no todas las clases pesan igual).\n",
    "\n",
    "- Te gu√≠a hacia una mejor interpretaci√≥n del modelo y ajustes m√°s efectivos.\n",
    "\n",
    "## üîç An√°lisis de errores en Clasificaci√≥n\n",
    "‚úÖ Proceso paso a paso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Entrena un modelo de clasificaci√≥n\n",
    "\n",
    "# 2. Eval√∫a con m√©tricas por clase\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# 3. Inspecciona la matriz de confusi√≥n normalizada (recall por fila)\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d52362",
   "metadata": {},
   "source": [
    "## üß≠ C√≥mo interpretar la matriz\n",
    "- Lectura por filas (con normalize=\"true\"):\n",
    "Cada fila representa una clase real ‚Üí el valor muestra a qu√© clase fue clasificada.\n",
    "\n",
    "- Las diagonales altas indican buen recall.\n",
    "\n",
    "- Las desviaciones fuera de la diagonal indican confusiones sistem√°ticas.\n",
    "\n",
    "üí° Ejemplo de an√°lisis cualitativo\n",
    "\n",
    "- Si clase 2 se clasifica mal y casi siempre como clase 4 o 5 ‚Üí hay un sesgo ascendente.\n",
    "\n",
    "- Si una clase con poco soporte tiene bajo recall, puedes:\n",
    "\n",
    "    -Reentrenar un modelo espec√≠fico para esa clase.\n",
    "\n",
    "    - Balancear las clases (oversampling).\n",
    "\n",
    "    - Aplicar un modelo de segundo nivel (\"modelo cascada\").\n",
    "\n",
    "\n",
    "## üìà Sugerencias para mejorar tras el an√°lisis\n",
    "1. Ingenier√≠a de variables\n",
    "Nuevas variables o transformar las existentes (escalado, codificaci√≥n ordinal...).\n",
    "\n",
    "2. Modelos especializados\n",
    "Para clases con errores sistem√°ticos o de alto impacto ‚Üí crear modelos dedicados.\n",
    "\n",
    "3. Reentrenar con class_weight\n",
    "Penalizar m√°s los errores en las clases minoritarias.\n",
    "\n",
    "4. Evaluar impacto real\n",
    "Discutir con negocio: ¬øcu√°l es el coste real de predecir mal cada clase?\n",
    "\n",
    "## üìâ An√°lisis de Errores en Regresi√≥n\n",
    "Cuando trabajamos con modelos de regresi√≥n, no podemos usar una matriz de confusi√≥n como en clasificaci√≥n, pero s√≠ podemos analizar los errores individuales (residuos), su distribuci√≥n y su relaci√≥n con las predicciones. Esto nos ayuda a identificar sesgos o zonas problem√°ticas.\n",
    "\n",
    "‚öôÔ∏è Preparaci√≥n del problema de regresi√≥n\n",
    "\n",
    "Se cambia el target a una variable num√©rica continua. En el ejemplo con el dataset diamonds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_regresion = \"price\"\n",
    "features_cat_reg = [\"color\", \"clarity\", \"cut\"]\n",
    "features_num_reg = [...]  # num√©ricas sin incluir \"price\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba28a9",
   "metadata": {},
   "source": [
    "Se aplica StandardScaler a las num√©ricas y luego se invierte el escalado del target para que vuelva a su escala real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"price\"] = scaler.inverse_transform(train_set[features_num])[..., price_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43aff5d",
   "metadata": {},
   "source": [
    "### üß™ M√©tricas de evaluaci√≥n en regresi√≥n\n",
    "| M√©trica                                  | Interpretaci√≥n                                                        |\n",
    "| ---------------------------------------- | --------------------------------------------------------------------- |\n",
    "| `MAE` (Error absoluto medio)             | Cu√°nto se equivoca el modelo de media (sin penalizar grandes errores) |\n",
    "| `MAPE` (Error porcentual medio)          | Cu√°nto se equivoca el modelo en proporci√≥n al valor real              |\n",
    "| `RMSE` (Ra√≠z del error cuadr√°tico medio) | Penaliza m√°s los errores grandes                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9765d4b",
   "metadata": {},
   "source": [
    "### üìä Visualizaciones esenciales\n",
    "1. Gr√°fico de reales vs predichos\n",
    "\n",
    "Permite detectar si el modelo funciona peor en ciertos rangos del target (por ejemplo, en precios altos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_vs_actual(y_real, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30cc936",
   "metadata": {},
   "source": [
    "Idealmente, los puntos deben estar alineados con la l√≠nea roja y = x.\n",
    "\n",
    "2. Distribuci√≥n de residuos\n",
    "\n",
    "Ayuda a ver si los errores tienen una distribuci√≥n sim√©trica o sesgada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos = y_test - y_pred\n",
    "sns.histplot(residuos, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48c9fc",
   "metadata": {},
   "source": [
    "Una distribuci√≥n normal de residuos suele indicar un modelo razonablemente ajustado.\n",
    "\n",
    "3. Residuos vs Predicciones\n",
    "\n",
    "Permite identificar zonas del rango de predicci√≥n donde el modelo sesga (por ejemplo, subestimando sistem√°ticamente):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred, residuos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801a23f",
   "metadata": {},
   "source": [
    "Un buen modelo deber√≠a mostrar los residuos dispersos alrededor del 0 sin patrones claros.\n",
    "\n",
    "### üß† ¬øQu√© hacer con la informaci√≥n del an√°lisis?\n",
    "\n",
    "En el ejemplo de diamonds, se observa que:\n",
    "\n",
    "El modelo funciona mejor en precios bajos.\n",
    "\n",
    "Para precios > 7500, hay mayor error y dispersi√≥n.\n",
    "\n",
    "El modelo parece sesgar m√°s en la \"cola larga\" de la distribuci√≥n.\n",
    "\n",
    "Esto puede indicar que:\n",
    "\n",
    "El modelo necesita mayor capacidad para manejar precios altos.\n",
    "\n",
    "Puede ser √∫til hacer un modelo espec√≠fico para ese segmento.\n",
    "\n",
    "Otra opci√≥n es binnear el target y convertirlo en clasificaci√≥n.\n",
    "\n",
    "### üí° Sugerencias de mejora\n",
    "\n",
    "1. Ingenier√≠a de caracter√≠sticas\n",
    "\n",
    "Transformar o crear nuevas features que ayuden al modelo.\n",
    "\n",
    "2. Modelos alternativos\n",
    "\n",
    "RandomForest, GradientBoosting, XGBoost...\n",
    "\n",
    "3. Ajuste de hiperpar√°metros\n",
    "\n",
    "Profundidad, n√∫mero de √°rboles, learning rate...\n",
    "\n",
    "4. Tratamiento de outliers\n",
    "\n",
    "Revisar los valores extremos que distorsionan la predicci√≥n.\n",
    "\n",
    "5. Segmentaci√≥n del modelo\n",
    "\n",
    "Crear un modelo exclusivo para valores altos o usar ensemble de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d649f4b",
   "metadata": {},
   "source": [
    "# SPRINT 13: ML: MODELOS NO SUPERVISADOS I. CLUSTERING\n",
    "\n",
    "## Clustering no supervisado\n",
    "\n",
    "Datos no etiquetados. Tenemos la **X**, las caracter√≠sticas de entrada, pero no tenemos la de salida **y** (el *target*). No podemos supervisar ni aplicar los algoritmos que hemos visto.\n",
    "\n",
    "El **aprendizaje no supervisado** es un m√©todo de *aprendizaje autom√°tico* donde el modelo se ajusta a las observaciones. Se distingue del supervisado porque **no hay conocimiento a priori**: los datos no est√°n etiquetados (no tienen *target*).\n",
    "\n",
    "El aprendizaje supervisado lo hemos aplicado a construir **cajas que permiten predecir o asignar una etiqueta**, que es precisamente lo que falta en los datasets no supervisados. Seg√∫n esa etiqueta, hablamos de problemas de **clasificaci√≥n** y de **regresi√≥n**.\n",
    "\n",
    "### Campos de aplicaci√≥n del aprendizaje no supervisado\n",
    "\n",
    "Tiene dos grandes campos de aplicaci√≥n:\n",
    "\n",
    "1. **An√°lisis cluster**  \n",
    "   Algoritmos cuyo objetivo es **crear grupos de instancias similares** no definidos por nosotros a priori.  \n",
    "   Gran herramienta para:\n",
    "   - An√°lisis de datos\n",
    "   - Segmentaci√≥n de clientes\n",
    "   - Sistemas de recomendaci√≥n\n",
    "   - Motores de b√∫squeda\n",
    "   - Segmentaci√≥n de im√°genes\n",
    "   - Aprendizaje semisupervisado\n",
    "\n",
    "2. **Reducci√≥n de dimensionalidad**  \n",
    "   Cuando tenemos miles de posibles *features*, surgen dos problemas:\n",
    "   - Modelado m√°s lento, tanto en la construcci√≥n como en la predicci√≥n.\n",
    "   - A medida que metemos m√°s *features*, aunque parezca que ayudamos al modelo, podemos dificultar que el algoritmo encuentre soluciones √≥ptimas (**maldici√≥n de la dimensionalidad**).\n",
    "\n",
    "   Una forma de aliviarlo es aplicar algoritmos que **transforman las *features* de entrada en un n√∫mero sensiblemente menor de *features***, combinando las anteriores, y que pueden tener igual o mayor poder predictivo, o mejorar los tiempos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fce5be",
   "metadata": {},
   "source": [
    "# 1. Clustering\n",
    "\n",
    "## ¬øQu√© es el clustering?\n",
    "\n",
    "El **clustering** agrupa datos **no etiquetados** en diferentes grupos (**clusters**) de forma autom√°tica.  \n",
    "El objetivo es que las instancias dentro de un mismo grupo sean lo m√°s similares posible, y las de diferentes grupos lo m√°s diferentes posible.\n",
    "\n",
    "Los algoritmos de clustering suelen utilizar como criterio la **distancia entre los puntos** en el espacio de caracter√≠sticas, considerando las *features* como componentes de un vector.\n",
    "\n",
    "### Clasificaci√≥n vs Clustering\n",
    "\n",
    "- En **clasificaci√≥n** tenemos los datos **etiquetados** y utilizamos algoritmos supervisados (Regresi√≥n Log√≠stica, SVC, Decision Tree...) para entrenar y predecir las clases.\n",
    "- En **clustering** no tenemos etiquetas. Es el propio algoritmo el que **crea los grupos** (k clusters) en base a similitudes. El modelo no sabe cu√°l es cada clase, solo agrupa observaciones similares.\n",
    "\n",
    "---\n",
    "\n",
    "## Aplicaciones del clustering\n",
    "\n",
    "- **Segmentaci√≥n de clientes**  \n",
    "  Clasificaci√≥n de clientes en grupos para sistemas recomendadores. No se definen los segmentos previamente; se dejan que el algoritmo agrupe y luego se decide c√≥mo actuar sobre cada grupo.\n",
    "\n",
    "- **Data Analysis**  \n",
    "  Muy √∫til para descubrir agrupaciones naturales en un dataset.  \n",
    "  Tambi√©n se puede usar para **crear nuevas *features*** en problemas supervisados, por ejemplo generando clusters autom√°ticos (por ejemplo con latitud y longitud) e incluyendo la clase del cluster como una nueva variable.\n",
    "\n",
    "- **Reducci√≥n de la dimensionalidad**  \n",
    "  Despu√©s de aplicar clustering, se puede representar cada observaci√≥n por su afinidad con los clusters, sustituyendo m√∫ltiples *features* por una nueva variable que indica el cluster al que pertenece.\n",
    "\n",
    "- **Detecci√≥n de anomal√≠as**  \n",
    "  Observaciones con **baja afinidad** respecto a su grupo pueden ser consideradas **anomal√≠as** o *outliers*.  \n",
    "  Ejemplo: detecci√≥n de comportamientos inusuales de usuarios.\n",
    "\n",
    "- **Algoritmos de b√∫squeda**  \n",
    "  B√∫squeda de im√°genes o elementos similares a una referencia, funcionando como un sistema de recomendaci√≥n basado en clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## Tipos de algoritmos de clustering\n",
    "\n",
    "### 1Ô∏è‚É£ Clustering no jer√°rquico\n",
    "\n",
    "Los grupos generados no tienen relaci√≥n jer√°rquica entre s√≠.  \n",
    "El algoritmo optimiza una funci√≥n de \"distancia\" a partir de unos **prototipos** (seleccionados aleatoriamente o manualmente).  \n",
    "El algoritmo parte de estos prototipos e iterativamente mejora el clustering.\n",
    "\n",
    "- **K-Means**  \n",
    "  Algoritmo basado en distancias. La suma de las distancias de los puntos al centroide de su cluster debe ser m√≠nima.\n",
    "\n",
    "- **DBSCAN**  \n",
    "  Define los clusters como regiones de **alta densidad** de observaciones, permitiendo detectar clusters de forma m√°s flexible y robusta a *outliers*.\n",
    "\n",
    "### 2Ô∏è‚É£ Clustering jer√°rquico\n",
    "\n",
    "Los clusters guardan una **relaci√≥n jer√°rquica** entre s√≠.  \n",
    "Se establece una distancia entre las propias clases, lo que permite construir un **dendrograma** que representa la estructura jer√°rquica.  \n",
    "Seg√∫n el punto de corte que pongamos en el dendrograma, obtendremos m√°s o menos clusters.\n",
    "\n",
    "- **Agrupaci√≥n aglomerativa (Bottom-Up)**  \n",
    "  Cada instancia empieza como un cluster individual, y se van agrupando las m√°s cercanas.\n",
    "\n",
    "- **Agrupaci√≥n divisiva (Top-Down)**  \n",
    "  Se parte de un **supercluster** que contiene todas las instancias, y se va dividiendo progresivamente en clusters m√°s peque√±os.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Resumen:**  \n",
    "El clustering es una herramienta muy potente del aprendizaje no supervisado, con aplicaciones en an√°lisis exploratorio, sistemas de recomendaci√≥n, reducci√≥n de dimensionalidad y detecci√≥n de anomal√≠as.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9b064",
   "metadata": {},
   "source": [
    "# 2. Reducci√≥n de dimensionalidad\n",
    "\n",
    "## ¬øQu√© es la reducci√≥n de dimensionalidad?\n",
    "\n",
    "Consiste en **reducir el n√∫mero de *features*** en un dataset, tratando de conservar la informaci√≥n relevante para el modelo.  \n",
    "Despu√©s de reducir la dimensionalidad, se compara si el nuevo dataset permite obtener modelos con igual o mejor rendimiento.\n",
    "\n",
    "Cada instancia en un dataset se representa como un **vector de n dimensiones** (n = n√∫mero de *features*).  \n",
    "A medida que a√±adimos m√°s *features*, aumentamos la **dimensionalidad** del dataset.\n",
    "\n",
    "### Efecto de la dimensionalidad\n",
    "\n",
    "- El n√∫mero de muestras necesarias para estimar una funci√≥n con un cierto nivel de precisi√≥n crece **exponencialmente** con el n√∫mero de dimensiones ‚Üí *maldici√≥n de la dimensionalidad*.\n",
    "- A m√°s dimensiones:\n",
    "  - Mejoran los tiempos de ejecuci√≥n.\n",
    "  - Los datos se dispersan m√°s en el espacio ‚Üí las distancias entre puntos aumentan.\n",
    "  - Inicialmente, esto puede mejorar la clasificaci√≥n o la capacidad de diferenciaci√≥n.\n",
    "  - Sin embargo, si las nuevas dimensiones no aportan informaci√≥n √∫til, aumenta el riesgo de **overfitting** ‚Üí nuevas observaciones quedan muy alejadas, produciendo predicciones poco fiables.\n",
    "  - **Soluci√≥n parcial:** incrementar el tama√±o del conjunto de entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## T√©cnicas de reducci√≥n de dimensionalidad\n",
    "\n",
    "### En modelos supervisados (con *target*)\n",
    "\n",
    "Ya vimos t√©cnicas de **selecci√≥n de *features***:\n",
    "\n",
    "- **Feature Selection:**  \n",
    "  Selecci√≥n basada en la correlaci√≥n entre las *features* y el *target*.  \n",
    "  Ejemplo: `feature_importances_`.\n",
    "\n",
    "- **Wrapper:**  \n",
    "  Ejemplo: `sklearn.feature_selection.RFE` (*Recursive Feature Elimination*).  \n",
    "  Va a√±adiendo o eliminando *features* iterativamente, evaluando el impacto en el rendimiento del modelo.\n",
    "\n",
    "- **Intrinsic (embedded):**  \n",
    "  Ejemplo: `sklearn.feature_selection.SelectFromModel`.  \n",
    "  Selecciona *features* a partir de la importancia que asigna un modelo previo.\n",
    "\n",
    "### En modelos no supervisados (sin *target*)\n",
    "\n",
    "- **PCA (An√°lisis de Componentes Principales):**  \n",
    "  T√©cnica de reducci√≥n de dimensionalidad que genera nuevas *features* (componentes principales) que explican la mayor parte de la varianza del dataset.\n",
    "\n",
    "- **Variance Threshold:**  \n",
    "  Elimina *features* con **varianza baja**, es decir, que aportan poca informaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## Aplicaciones de la reducci√≥n de dimensionalidad\n",
    "\n",
    "- **Mejora computacional**  \n",
    "  Modelos m√°s r√°pidos de entrenar y predecir.\n",
    "\n",
    "- **Detecci√≥n de *features* discriminantes**  \n",
    "  Identificaci√≥n de las variables que m√°s valor aportan al modelo.\n",
    "\n",
    "- **Eliminaci√≥n de *features* irrelevantes**  \n",
    "  Mejora la interpretabilidad del modelo.\n",
    "\n",
    "- **Compresi√≥n de la informaci√≥n**  \n",
    "  Por ejemplo, en el procesamiento de im√°genes.\n",
    "\n",
    "- **Visualizaci√≥n de datos**  \n",
    "  Ejemplo: reducci√≥n a 2 o 3 dimensiones mediante **PCA** para facilitar la exploraci√≥n visual.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Resumen:**  \n",
    "Reducir la dimensionalidad permite **simplificar el modelo**, mejorar tiempos de ejecuci√≥n y combatir la *maldici√≥n de la dimensionalidad*, siempre procurando no perder la informaci√≥n clave del problema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d62a11",
   "metadata": {},
   "source": [
    "### Introducci√≥n ‚Äì Clasificaci√≥n _vs_ Agrupamiento (Agrupamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f658588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4236c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca21c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b98537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x350 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9, 3.5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X[y==0, 2], X[y==0, 3], \"yo\", label=\"Iris setosa\")\n",
    "plt.plot(X[y==1, 2], X[y==1, 3], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y==2, 2], X[y==2, 3], \"g^\", label=\"Iris virginica\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 2], X[:, 3], c=\"k\", marker=\".\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.tick_params(labelleft=False)\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9f068",
   "metadata": {},
   "source": [
    "El dataset representado a la izquierda es el que usar√≠amos con un modelo supervisado para hacer la clasificaci√≥n. El dataset representado a la derecha es, en cambio, el que tendr√≠amos si no tuvieramos etiquetas y el que entregar√≠amos a un algoritmo no supervisado como kmeans. Es lo que vamos a hacer en la siguiente celda (ojo es solo a t√≠tulo ilustrativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=101)\n",
    "\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Los 0s del kmeans en realidad son los 1s del dataset y viceversa (ajuste para comparar)\n",
    "mapping = np.array([1, 0, 2])\n",
    "y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15bb69",
   "metadata": {},
   "source": [
    "Pintemos la salida y compar√©mosla al caso supervisado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8ac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], \"yo\", label=\"Cluster 1\")\n",
    "plt.plot(X[y_pred==1, 2], X[y_pred==1, 3], \"bs\", label=\"Cluster 2\")\n",
    "plt.plot(X[y_pred==2, 2], X[y_pred==2, 3], \"g^\", label=\"Cluster 3\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a483910b",
   "metadata": {},
   "source": [
    "No lo ha hecho nada mal. Pero ojo, antes de que se te haga un l√≠o el cerebro, fijate en las etiquetas de la leyenda. El algoritmo ha encontrado una agrupaci√≥n con esos nombres, no puede decir si corresponden a un tipo u otro de flor, s√≥lo lo hacemos nosotros porque tenemos la versi√≥n \"supervisada\". En un realidad en la que no la tuvieramos s√≥lo tendr√≠amos esa clasificaci√≥n (la de la imagen) y no su explicaci√≥n. A partir de ah√≠ trabajar√≠amos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fadf904",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e29be9",
   "metadata": {},
   "source": [
    "## ‚ú® Introducci√≥n\n",
    "\n",
    "El algoritmo **K-means** es un algoritmo de *clustering* sencillo, r√°pido y eficiente, capaz de agrupar datos en **k clusters** en pocas iteraciones.\n",
    "\n",
    "- Propuesto por **Stuart Lloyd** en Bell Labs en 1957, publicado en 1982.\n",
    "- Publicado de forma independiente por **Edward W. Forgy** en 1965 ‚Üí a veces llamado *algoritmo Lloyd-Forgy*.\n",
    "\n",
    "üëâ **K** es un **hiperpar√°metro** que indica el n√∫mero de clusters que queremos generar. Tendremos que buscar una forma de optimizar este valor (lo veremos m√°s adelante).\n",
    "\n",
    "---\n",
    "\n",
    "## üåÄ ¬øQu√© es un Centroide?\n",
    "\n",
    "El **centroide** es el **centro de un cluster**. \n",
    "\n",
    "- Es el punto que representa el centroide de los \"manchurrones\" de datos que queremos crear.\n",
    "- Se calcula como el **promedio de cada feature** de los puntos que pertenecen al cluster.\n",
    "\n",
    "### üìê Definici√≥n formal\n",
    "\n",
    "En geometr√≠a, el centroide o baricentro de un conjunto de puntos $(x_i, y_i, z_i)$ se calcula como:\n",
    "\n",
    "$$\n",
    "C_x = \\frac{1}{n}\\sum_{i=1}^{n} x_i, \\quad \n",
    "C_y = \\frac{1}{n}\\sum_{i=1}^{n} y_i, \\quad \n",
    "C_z = \\frac{1}{n}\\sum_{i=1}^{n} z_i\n",
    "$$\n",
    "\n",
    "En Machine Learning:\n",
    "\n",
    "- Un **centroide** es el punto que representa el centro de un cluster.\n",
    "- Se recalcula en cada iteraci√≥n de K-means a partir del promedio de todas las observaciones que pertenecen a ese cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Funcionamiento b√°sico de K-means\n",
    "\n",
    "1. Elegimos el n√∫mero de clusters **K**.\n",
    "2. Inicializamos aleatoriamente **K centroides**.\n",
    "3. Repetimos hasta converger:\n",
    "    - Asignamos cada punto de datos al centroide m√°s cercano (formando clusters).\n",
    "    - Recalculamos los centroides como el promedio de los puntos de cada cluster.\n",
    "\n",
    "üëâ El algoritmo termina cuando los centroides no cambian o cambian muy poco.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è C√≥digo base para aplicar K-means en Python\n",
    "\n",
    "```python\n",
    "# Importamos las librer√≠as necesarias\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Ejemplo: vamos a suponer que tenemos un DataFrame llamado df con las columnas num√©ricas a clusterizar\n",
    "# Supongamos que queremos hacer clustering sobre las columnas 'feature1' y 'feature2'\n",
    "\n",
    "X = df[['feature1', 'feature2']]  # seleccionamos las columnas num√©ricas\n",
    "\n",
    "# Definimos el modelo con K clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Ajustamos el modelo\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Obtenemos las predicciones (a qu√© cluster pertenece cada punto)\n",
    "df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Obtenemos las coordenadas de los centroides\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Visualizamos los clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='feature1', y='feature2', hue='cluster', palette='Set1')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='black', marker='X', label='Centroides')\n",
    "plt.legend()\n",
    "plt.title('Clusters obtenidos con K-means')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b27f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üó∫Ô∏è L√≠mites de decisi√≥n y diagrama de Voronoi\n",
    "\n",
    "Una de las formas m√°s interesantes de visualizar los resultados de un K-means es trazar los **l√≠mites de decisi√≥n** entre los clusters.\n",
    "\n",
    "üëâ Esta visualizaci√≥n se llama **diagrama de Voronoi**: nos muestra qu√© regiones del espacio de datos corresponden a cada cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### üö¶ Concepto clave\n",
    "\n",
    "- Los **l√≠mites de decisi√≥n** indican las zonas del espacio que se asignan a cada centroide.\n",
    "- Cada punto del espacio se asigna al **centroide m√°s cercano** ‚Üí esta es la l√≥gica que sigue el K-means.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è C√≥digo: funciones auxiliares\n",
    "\n",
    "Estas funciones NO son fundamentales para el aprendizaje del algoritmo, pero s√≠ muy √∫tiles para visualizar los resultados.\n",
    "\n",
    "```python\n",
    "# Funci√≥n para dibujar los puntos de datos\n",
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "# Funci√≥n para dibujar los centroides\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='b'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=30, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=15, linewidths=20,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "# Funci√≥n para trazar los l√≠mites de decisi√≥n (diagrama de Voronoi)\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    # Definimos el grid de puntos\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    \n",
    "    # Predecimos a qu√© cluster pertenece cada punto del grid\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Dibujamos las regiones\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors='k')\n",
    "\n",
    "    # Dibujamos los puntos de datos\n",
    "    plot_data(X)\n",
    "\n",
    "    # Dibujamos los centroides (opcional)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    # Etiquetas de los ejes\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c84d152",
   "metadata": {},
   "source": [
    "# Hard Clustering vs. Soft Clustering\n",
    "\n",
    "En el contexto del clustering, existen dos enfoques principales para asignar instancias a cl√∫steres:\n",
    "\n",
    "## Hard Clustering\n",
    "\n",
    "- Consiste en asignar cada instancia **al cl√∫ster m√°s cercano**.\n",
    "- Es un enfoque **determinista**: cada instancia pertenece a un √∫nico cl√∫ster.\n",
    "- Es lo que hace el m√©todo `kmeans.predict()`, que devuelve la etiqueta del cl√∫ster m√°s cercano para cada instancia.\n",
    "\n",
    "```python\n",
    "# Ejemplo de Hard Clustering\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)\n",
    "# Resultado: array([2, 2, 1, 1], dtype=int32)\n",
    "```\n",
    "\n",
    "üîπ Este comportamiento es an√°logo al hard voting en los ensamblados (ensembles).\n",
    "\n",
    "## Soft Clustering\n",
    "\n",
    "- En lugar de asignar una √∫nica etiqueta, se calcula la distancia a cada centroide.\n",
    "- Esto permite tener una idea de la cercan√≠a relativa a todos los cl√∫steres.\n",
    "- El m√©todo kmeans.transform() devuelve precisamente esta informaci√≥n.\n",
    "\n",
    "```python\n",
    "# Ejemplo de Soft Clustering\n",
    "kmeans.transform(X_new)\n",
    "# Resultado:\n",
    "array([[2.88468831, 2.90886155, 0.35083255, 1.51889559, 2.80754989],\n",
    "       [5.84017149, 5.85069849, 2.812728  , 4.49697731, 5.80394207],\n",
    "       [1.71505482, 0.28210043, 3.28038973, 1.6619582 , 1.21436711],\n",
    "       [1.219972  , 0.36868613, 3.20902516, 1.5253504 , 0.72581567]])\n",
    "```\n",
    "\n",
    "- Estas distancias se pueden usar como features adicionales en un modelo.\n",
    "\n",
    "- Tambi√©n es una buena forma de reducir dimensionalidad: se pasa de n variables originales a k distancias.\n",
    "\n",
    "üîπ Este comportamiento es an√°logo al soft voting en los ensamblados (ensembles).\n",
    "\n",
    "Verificaci√≥n: C√°lculo manual de distancias\n",
    "\n",
    "Puedes comprobar que kmeans.transform() realmente devuelve las distancias euclidianas entre las instancias y los centroides:\n",
    "\n",
    "```python\n",
    "np.linalg.norm(\n",
    "    np.tile(X_new, (1, k)).reshape(-1, k, 2) - kmeans.cluster_centers_, \n",
    "    axis=2\n",
    ")\n",
    "```\n",
    "\n",
    "# Resultado (mismo que transform()):\n",
    "\n",
    "```python\n",
    "array([[2.88468831, 2.90886155, 0.35083255, 1.51889559, 2.80754989],\n",
    "       [5.84017149, 5.85069849, 2.812728  , 4.49697731, 5.80394207],\n",
    "       [1.71505482, 0.28210043, 3.28038973, 1.6619582 , 1.21436711],\n",
    "       [1.219972  , 0.36868613, 3.20902516, 1.5253504 , 0.72581567]])\n",
    "```\n",
    "\n",
    "Explicaci√≥n:\n",
    "- np.tile() y reshape permiten repetir X_new tantas veces como centroides haya (k).\n",
    "\n",
    "- Se calcula la norma euclidiana entre cada punto y cada centroide.\n",
    "\n",
    "- El resultado es una matriz de shape (n_instancias, k).\n",
    "\n",
    "| Tipo de Clustering | Salida Principal            | Uso                                             |\n",
    "| ------------------ | --------------------------- | ----------------------------------------------- |\n",
    "| Hard Clustering    | Cl√∫ster asignado (etiqueta) | Clasificaci√≥n pura                              |\n",
    "| Soft Clustering    | Distancias a los centroides | Reducci√≥n de dimensionalidad, an√°lisis avanzado |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2ab07",
   "metadata": {},
   "source": [
    "# Algoritmo K-Means\n",
    "\n",
    "## Funcionamiento te√≥rico\n",
    "\n",
    "El algoritmo K-Means es uno de los algoritmos de agrupamiento m√°s r√°pidos y simples.\n",
    "\n",
    "Pasos b√°sicos:\n",
    "\n",
    "1. **Inicializaci√≥n**:  \n",
    "   Se eligen aleatoriamente $k$ instancias del dataset como centroides iniciales.\n",
    "\n",
    "2. **Asignaci√≥n**:  \n",
    "   Se asigna cada instancia al cl√∫ster cuyo centroide est√© m√°s cercano.\n",
    "\n",
    "3. **Actualizaci√≥n**:  \n",
    "   Se recalculan los centroides como la media de las instancias asignadas a cada cl√∫ster.\n",
    "\n",
    "4. **Repetici√≥n**:  \n",
    "   Se repiten los pasos 2 y 3 hasta que los centroides converjan (dejen de moverse m√°s all√° de una tolerancia) o se alcance el n√∫mero m√°ximo de iteraciones.\n",
    "\n",
    "### Notas:\n",
    "\n",
    "- La clase `KMeans` de `sklearn` implementa una versi√≥n optimizada.  \n",
    "- Para forzar el comportamiento \"original\" con inicializaci√≥n aleatoria: `init=\"random\"` y `n_init=1`.\n",
    "\n",
    "---\n",
    "\n",
    "## Ejemplo: Evoluci√≥n de los centroides\n",
    "\n",
    "Vamos a ejecutar K-Means con `max_iter=1`, `2`, y `3`, para visualizar c√≥mo evolucionan los centroides:\n",
    "\n",
    "```python\n",
    "# Inicializaci√≥n\n",
    "kmeans_iter1 = KMeans(n_clusters=5, init='random', n_init=1, max_iter=1, random_state=24)\n",
    "kmeans_iter2 = KMeans(n_clusters=5, init='random', n_init=1, max_iter=2, random_state=24)\n",
    "kmeans_iter3 = KMeans(n_clusters=5, init='random', n_init=1, max_iter=3, random_state=24)\n",
    "\n",
    "# Entrenamiento\n",
    "kmeans_iter1.fit(X)\n",
    "kmeans_iter2.fit(X)\n",
    "kmeans_iter3.fit(X)\n",
    "```\n",
    "\n",
    "## Hiperpar√°metros relevantes\n",
    "\n",
    "```python\n",
    "init      # M√©todo de inicializaci√≥n de los centroides. 'random' los elige aleatoriamente.\n",
    "n_init    # N√∫mero de veces que se repite el algoritmo con diferentes inicializaciones.\n",
    "max_iter  # N√∫mero m√°ximo de iteraciones para recalcular centroides.\n",
    "```\n",
    "\n",
    "## Visualizaci√≥n\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Iteraci√≥n 1: centroides iniciales y zonas de decisi√≥n\n",
    "plt.subplot(321)\n",
    "plot_data(X)\n",
    "plot_centroids(kmeans_iter1.cluster_centers_)\n",
    "plt.title(\"Iteraci√≥n 1 - Centroides iniciales\")\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_decision_boundaries(kmeans_iter1, X)\n",
    "plt.title(\"Iteraci√≥n 1 - Etiquetado de instancias\")\n",
    "\n",
    "# Iteraci√≥n 2\n",
    "plt.subplot(323)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_centroids=False)\n",
    "plot_centroids(kmeans_iter2.cluster_centers_)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_decision_boundaries(kmeans_iter2, X)\n",
    "\n",
    "# Iteraci√≥n 3\n",
    "plt.subplot(325)\n",
    "plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\n",
    "plot_centroids(kmeans_iter3.cluster_centers_)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_decision_boundaries(kmeans_iter3, X)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Variabilidad de K-Means\n",
    "Como los centroides iniciales son aleatorios, K-Means puede converger a soluciones distintas en diferentes ejecuciones\n",
    "\n",
    "```python\n",
    "# Definimos funci√≥n para comparar dos clusterers\n",
    "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n",
    "    clusterer1.fit(X)\n",
    "    clusterer2.fit(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plot_decision_boundaries(clusterer1, X)\n",
    "    if title1:\n",
    "        plt.title(title1)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_decision_boundaries(clusterer2, X)\n",
    "    if title2:\n",
    "        plt.title(title2)\n",
    "```\n",
    "\n",
    "## Ejemplo con dos inicializaciones distintas:\n",
    "```python\n",
    "kmeans_rnd_init1 = KMeans(n_clusters=5, init='random', n_init=1, random_state=42)\n",
    "kmeans_rnd_init2 = KMeans(n_clusters=5, init='random', n_init=1, random_state=141)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n",
    "                          \"Soluci√≥n 1 (rs=42)\", \"Soluci√≥n 2 (rs=141)\")\n",
    "plt.show()\n",
    "```\n",
    "¬øCu√°l es la mejor soluci√≥n?\n",
    "\n",
    "La mejor se suele escoger en base a la inercia (inertia), que mide qu√© tan compactos son los cl√∫steres.\n",
    "\n",
    "Por defecto, KMeans repite el proceso (n_init) varias veces y devuelve la soluci√≥n con menor inercia.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb1b75",
   "metadata": {},
   "source": [
    "# Inercia (Inertia)\n",
    "\n",
    "Cuando usamos K-Means, necesitamos una forma de evaluar la calidad del clustering.\n",
    "\n",
    "üëâ Como el clustering es un problema **no supervisado**, no tenemos m√©tricas como *precisi√≥n* o *recall*.  \n",
    "Sin embargo, s√≠ podemos medir la **distancia entre cada instancia y su centroide asignado**.\n",
    "\n",
    "Esto es la idea detr√°s de la m√©trica de **_inercia_** (*inertia* en ingl√©s).\n",
    "\n",
    "---\n",
    "\n",
    "## Definici√≥n de Inercia\n",
    "\n",
    "> **Inercia = suma de las distancias al cuadrado de cada punto a su centroide m√°s cercano.**\n",
    "\n",
    "- Cuanto **menor** es la inercia, m√°s compactos son los cl√∫steres ‚Üí agrupaci√≥n \"mejor\".\n",
    "- No garantiza clusters \"√≥ptimos\", pero es una m√©trica de referencia.\n",
    "\n",
    "```python\n",
    "# Obtener la inercia directamente\n",
    "kmeans_rnd_init1.inertia_\n",
    "# Resultado: 266.6059795469669\n",
    "\n",
    "kmeans_rnd_init2.inertia_\n",
    "# Resultado: 625.7421301887271\n",
    "```\n",
    "### Interpretaci√≥n\n",
    "En este ejemplo, el primer modelo tiene menor inercia ‚Üí mejor agrupaci√≥n.\n",
    "\n",
    "## Verificaci√≥n manual\n",
    "La inercia es realmente la suma de las distancias al cuadrado, y podemos comprobarlo:\n",
    "\n",
    "```python\n",
    "# Distancias de cada instancia a cada centroide\n",
    "X_dist = kmeans_rnd_init1.transform(X)\n",
    "\n",
    "# Suma de las distancias al cuadrado a su centroide asignado\n",
    "sum(X_dist[np.arange(len(X_dist)), kmeans_rnd_init1.labels_]**2)\n",
    "\n",
    "# Resultado: 266.6059795469672\n",
    "```\n",
    "\n",
    "## Calculo alternativo (manual)\n",
    "\n",
    "```python\n",
    "inercia = 0\n",
    "for indice, vector in enumerate(X):\n",
    "    clase_asignada = kmeans_rnd_init2.labels_[indice]\n",
    "    distancia = np.linalg.norm(vector - kmeans_rnd_init2.cluster_centers_[clase_asignada])\n",
    "    inercia += distancia**2\n",
    "\n",
    "print(inercia)\n",
    "# Resultado: 625.7421301887265\n",
    "```\n",
    "\n",
    "## Inercia y score()\n",
    "- La clase KMeans tambi√©n ofrece el m√©todo score(), que devuelve la inercia negativa.\n",
    "\n",
    "- ¬øPor qu√© negativa? ‚Üí porque por convenci√≥n en sklearn, el m√©todo score() siempre debe ser \"cuanto m√°s grande, mejor\".\n",
    "\n",
    "```python\n",
    "# Score (inercia negativa)\n",
    "kmeans_rnd_init1.score(X)\n",
    "# Resultado: -266.60597954696686\n",
    "```\n",
    "## Utilidad de la Inercia\n",
    "- Nos permite comparar distintas ejecuciones de K-Means con diferentes inicializaciones.\n",
    "\n",
    "- Nos ayuda a mitigar la dependencia de la inicializaci√≥n aleatoria.\n",
    "\n",
    "- Se usa en t√©cnicas como el m√©todo del codo (elbow method) para elegir el n√∫mero √≥ptimo de cl√∫steres.\n",
    "\n",
    "üëâ Objetivo: minimizar la inercia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042471f4",
   "metadata": {},
   "source": [
    "# M√∫ltiples Inicializaciones en K-Means\n",
    "\n",
    "## Problema de Variabilidad\n",
    "\n",
    "K-Means puede converger a soluciones diferentes en funci√≥n de la inicializaci√≥n aleatoria de los centroides.\n",
    "\n",
    "üëâ Esto puede llevar a resultados sub√≥ptimos, con cl√∫steres mal formados.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```python\n",
    "kmeans_rnd_init1.inertia_  # 266.61\n",
    "kmeans_rnd_init2.inertia_  # 625.74\n",
    "kmeans.inertia_            # 218.53  ‚Üí mejor soluci√≥n\n",
    "```\n",
    "## Soluci√≥n: M√∫ltiples Inicializaciones\n",
    "Para mitigar este problema, es recomendable ejecutar el algoritmo varias veces con diferentes inicializaciones y quedarse con la mejor soluci√≥n (la de menor inercia).\n",
    "\n",
    "Esto se controla con el hiperpar√°metro:\n",
    "```python\n",
    "n_init  # N√∫mero de veces que se repite el algoritmo con distintas inicializaciones.\n",
    "```\n",
    "\n",
    "Por defecto en sklearn, n_init=10.\n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "kmeans_rnd_10_init = KMeans(n_clusters=5, init='random', n_init=10, random_state=29)\n",
    "kmeans_rnd_10_init.fit(X)\n",
    "```\n",
    "\n",
    "üëâ El algoritmo ejecuta 10 inicializaciones ‚Üí selecciona la que minimiza la inercia.\n",
    "\n",
    "## Visualizaci√≥n de la soluci√≥n\n",
    "```python\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans_rnd_10_init, X)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Importancia de la Inercia\n",
    "- Internamente, la inercia permite que KMeans seleccione la mejor ejecuci√≥n tras m√∫ltiples inicializaciones.\n",
    "\n",
    "-  Adem√°s, la inercia es √∫til para encontrar el n√∫mero de cl√∫steres √≥ptimo (k), mediante el conocido: M√©todo del Codo (Elbow Method) ‚Üí se explica en la siguiente secci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517be9f",
   "metadata": {},
   "source": [
    "# Encontrar el n√∫mero √≥ptimo de cl√∫steres\n",
    "\n",
    "## M√©todo basado en la inercia\n",
    "\n",
    "Como la inercia mide la compacidad del clustering, parece razonable intentar encontrar el n√∫mero de cl√∫steres $k$ que minimice la inercia.\n",
    "\n",
    "Sin embargo:\n",
    "\n",
    "- **A m√°s cl√∫steres, menor inercia** ‚Üí pero no siempre m√°s cl√∫steres significan mejor agrupaci√≥n.\n",
    "- Si simplemente minimizamos la inercia, acabar√≠amos con tantos cl√∫steres como instancias.\n",
    "\n",
    "üëâ Por eso se utiliza el **M√©todo del Codo** (*Elbow Method*).\n",
    "\n",
    "---\n",
    "\n",
    "## Ejemplo visual\n",
    "\n",
    "Probamos diferentes valores de $k$ y observamos:\n",
    "\n",
    "```python\n",
    "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n",
    "plt.show()\n",
    "\n",
    "# Inercias\n",
    "kmeans_k3.inertia_  # 645.15\n",
    "kmeans_k8.inertia_  # 136.63\n",
    "```\n",
    "\n",
    "üëâ La inercia disminuye con $k$, pero no siempre el modelo resultante es bueno.\n",
    "\n",
    "## M√©todo del Codo (Elbow Method)\n",
    "En lugar de elegir simplemente el $k$ con menor inercia, graficamos inercia vs. n√∫mero de cl√∫steres y buscamos el \"codo\" de la curva:\n",
    "\n",
    "```python\n",
    "# Entrenamos modelos con diferentes k\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42, n_init=25).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "\n",
    "# Recogemos las inercias\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "# Graficamos\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow/Codo',\n",
    "             xy=(4, inertias[3]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1))\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "plt.show()\n",
    "```\n",
    "Interpretaci√≥n:\n",
    "- A partir de cierto $k$, la mejora en la inercia se vuelve marginal.\n",
    "\n",
    "- Ese \"codo\" es el punto de compromiso: buen clustering sin sobreajustar.\n",
    "\n",
    "üëâ En el ejemplo, el codo aparece en $k=4$.\n",
    "\n",
    "## Visualizaci√≥n de la soluci√≥n con $k=4$\n",
    "```python\n",
    "plot_decision_boundaries(kmeans_per_k[4-1], X)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Observaci√≥n:\n",
    "- $k=4$ ofrece un buen equilibrio.\n",
    "\n",
    "- M√°s cl√∫steres no mejorar√≠an significativamente el agrupamiento, y menos cl√∫steres ser√≠an claramente insuficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8bc5c1",
   "metadata": {},
   "source": [
    "# Encontrar el n√∫mero √≥ptimo de cl√∫steres\n",
    "\n",
    "## Mediante las siluetas\n",
    "\n",
    "### Coeficiente de Silueta\n",
    "\n",
    "El **coeficiente de silueta** mide, para cada instancia, qu√© tan bien est√° agrupada:\n",
    "\n",
    "$$(b - a) / \\max(a, b)$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $a$ = distancia media a las instancias del mismo cl√∫ster (intra-cl√∫ster).\n",
    "- $b$ = distancia media al cl√∫ster m√°s cercano (inter-cl√∫ster).\n",
    "\n",
    "Interpretaci√≥n:\n",
    "\n",
    "| Valor del coeficiente | Interpretaci√≥n |\n",
    "|-----------------------|----------------|\n",
    "| Cerca de **+1**       | Instancia bien agrupada |\n",
    "| Cerca de **0**        | Instancia cerca del borde entre cl√∫steres |\n",
    "| Cerca de **-1**       | Instancia probablemente mal asignada |\n",
    "\n",
    "üëâ En la pr√°ctica, se usa el **silhouette_score**: el promedio de los coeficientes de todas las instancias.\n",
    "\n",
    "### C√°lculo del Silhouette Score\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(X, kmeans.labels_)\n",
    "# Ejemplo: 0.6495\n",
    "```\n",
    "\n",
    "## Silhouette Score para varios k\n",
    "Probamos distintos valores de $k$ y graficamos:\n",
    "\n",
    "```python\n",
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
    "plt.show()\n",
    "```\n",
    "üëâ Esta visualizaci√≥n permite ver qu√© valores de $k$ dan un mejor clustering.\n",
    "\n",
    "En el ejemplo:\n",
    "\n",
    "- $k=4$ y $k=5$ son buenas elecciones ‚Üí valores de silueta altos y estables.\n",
    "- $k$ muy bajo o muy alto deteriora la calidad.\n",
    "\n",
    "## Diagrama de Silueta (Silhouette Diagram)\n",
    "Visualiza el coeficiente de silueta de cada instancia, ordenadas por cl√∫ster y valor de coeficiente:\n",
    "```python\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "for k in (3, 4, 5, 6):\n",
    "    plt.subplot(2, 2, k - 2)\n",
    "    \n",
    "    y_pred = kmeans_per_k[k - 1].labels_\n",
    "    silhouette_coefficients = silhouette_samples(X, y_pred)\n",
    "\n",
    "    padding = len(X) // 30\n",
    "    pos = padding\n",
    "    ticks = []\n",
    "    for i in range(k):\n",
    "        coeffs = silhouette_coefficients[y_pred == i]\n",
    "        coeffs.sort()\n",
    "\n",
    "        color = mpl.cm.Spectral(i / k)\n",
    "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ticks.append(pos + len(coeffs) // 2)\n",
    "        pos += len(coeffs) + padding\n",
    "\n",
    "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "\n",
    "    if k in (3, 5):\n",
    "        plt.ylabel(\"Cluster\")\n",
    "    if k in (5, 6):\n",
    "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        plt.xlabel(\"Silhouette Coefficient\")\n",
    "\n",
    "    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(f\"$k={k}$\", fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Interpretaci√≥n:\n",
    "\n",
    "- Cada \"cuchillo\" es un cl√∫ster ‚Üí su grosor indica el n√∫mero de muestras.\n",
    "- El coeficiente deber√≠a ser alto y consistente dentro de cada cl√∫ster.\n",
    "- Instancias con coeficiente negativo ‚Üí probablemente mal asignadas.\n",
    "\n",
    "üëâ En el ejemplo, $k=4$ y $k=5$ son buenas opciones.\n",
    "\n",
    "## L√≠mites de K-Means\n",
    "K-Means asume:\n",
    "\n",
    "- Cl√∫steres convexos y aproximadamente esf√©ricos.\n",
    "\n",
    "- Distribuci√≥n homog√©nea.\n",
    "\n",
    "Cuando esta suposici√≥n no se cumple ‚Üí K-Means puede fallar.\n",
    "\n",
    "Ejemplo con cl√∫steres de forma irregular:\n",
    "\n",
    "```python\n",
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]\n",
    "\n",
    "plot_clusters(X)\n",
    "```\n",
    "\n",
    "Comparaci√≥n:\n",
    "\n",
    "```python\n",
    "# Modelo con centroides bien elegidos manualmente\n",
    "kmeans_good = KMeans(n_clusters=3,\n",
    "                     init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]),\n",
    "                     n_init=1,\n",
    "                     random_state=42)\n",
    "\n",
    "# Modelo con inicializaci√≥n aleatoria\n",
    "kmeans_bad = KMeans(n_clusters=3, random_state=19)\n",
    "\n",
    "kmeans_good.fit(X)\n",
    "kmeans_bad.fit(X)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_decision_boundaries(kmeans_good, X)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_good.inertia_), fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_bad.inertia_), fontsize=14)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "Observaciones:\n",
    "\n",
    "- La inercia puede ser m√°s baja aunque el clustering sea peor si la forma de los datos no se adapta bien a las suposiciones de K-Means.\n",
    "- En este ejemplo, la visualizaci√≥n de fronteras de decisi√≥n es clave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033847e",
   "metadata": {},
   "source": [
    "# Clustering No Jer√°rquico (II): DBSCAN\n",
    "\n",
    "## Introducci√≥n\n",
    "\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de clustering basado en la densidad, es decir, agrupa puntos que est√°n muy juntos entre s√≠ y detecta zonas de baja densidad como *ruido* o *outliers*.\n",
    "\n",
    "### Ventajas principales:\n",
    "\n",
    "‚úÖ No requiere definir el n√∫mero de clusters.  \n",
    "‚úÖ Puede encontrar clusters de cualquier forma (no necesita que sean esf√©ricos como k-means).  \n",
    "‚úÖ Identifica f√°cilmente outliers o anomal√≠as.\n",
    "\n",
    "### Limitaciones:\n",
    "\n",
    "‚ö†Ô∏è La elecci√≥n de `eps` y `min_samples` es muy importante.  \n",
    "‚ö†Ô∏è No funciona bien si la densidad var√≠a mucho entre clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## C√≥mo funciona DBSCAN\n",
    "\n",
    "DBSCAN define los grupos como **regiones continuas de alta densidad**. Su funcionamiento paso a paso es el siguiente:\n",
    "\n",
    "1. Para cada punto, cuenta cu√°ntos puntos est√°n dentro de una distancia `eps` (Œµ-neighborhood).\n",
    "2. Si un punto tiene al menos `min_samples` en su vecindario (incluy√©ndose a s√≠ mismo), es una **instancia core** (core instance).\n",
    "3. Todas las instancias dentro del vecindario de una instancia core pertenecen al mismo cluster. Los vecindarios se van fusionando a medida que se conectan entre s√≠.\n",
    "4. Los puntos que no son core y no est√°n en el vecindario de ninguna core se consideran **anomal√≠as** (etiquetadas como `-1`).\n",
    "\n",
    "### Par√°metros clave\n",
    "\n",
    "- `eps`: distancia m√°xima para considerar que un punto es vecino de otro.\n",
    "- `min_samples`: n√∫mero m√≠nimo de puntos para formar un cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## Ejemplo pr√°ctico\n",
    "\n",
    "### 1Ô∏è‚É£ Configuraci√≥n inicial\n",
    "\n",
    "```python\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Estabilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot bonito\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Ignorar warnings in√∫tiles\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "```\n",
    "\n",
    "### Interpretaci√≥n de los resultados\n",
    "Con eps peque√±o, se identifican muchos clusters y tambi√©n anomal√≠as.\n",
    "\n",
    "Al aumentar eps, los vecindarios crecen y los clusters se consolidan en formas m√°s amplias. Se pueden conseguir agrupaciones m√°s limpias y con menos anomal√≠as."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12300ca2",
   "metadata": {},
   "source": [
    "## \"Predicciones\" con DBSCAN\n",
    "\n",
    "### Limitaci√≥n del m√©todo `.predict()`\n",
    "\n",
    "Una curiosidad de DBSCAN es que **no tiene un m√©todo `.predict()`**.  \n",
    "S√≠ que tiene un m√©todo `.fit_predict()`, que permite etiquetar el conjunto de datos con el que fue entrenado (train), pero **no permite directamente predecir la etiqueta de nuevas instancias que no estaban en el entrenamiento**.\n",
    "\n",
    "üëâ Esto tiene sentido porque DBSCAN es un algoritmo basado en la densidad de los datos observados inicialmente. Si se a√±aden nuevos puntos, habr√≠a que \"rean√°lisis\" la densidad, cosa que el modelo no hace por dise√±o.\n",
    "\n",
    "---\n",
    "\n",
    "### Soluci√≥n pr√°ctica: combinar DBSCAN con un clasificador supervisado\n",
    "\n",
    "Lo que s√≠ podemos hacer es **usar las etiquetas que nos ha dado DBSCAN como si fueran etiquetas de clasificaci√≥n**.\n",
    "\n",
    "Por tanto:\n",
    "\n",
    "- Despu√©s de entrenar el DBSCAN, tenemos nuestro dataset etiquetado.\n",
    "- Ahora podemos usar ese dataset etiquetado para entrenar cualquier algoritmo de clasificaci√≥n (KNN, RandomForest, SVM, etc).\n",
    "- De este modo s√≠ podremos **clasificar nuevas instancias** que lleguen en el futuro.\n",
    "\n",
    "### Elecci√≥n del conjunto de entrenamiento\n",
    "\n",
    "Cuando entrenamos el clasificador, podemos elegir qu√© datos usar:\n",
    "\n",
    "- **Solo las instancias core** (core instances): m√°s fiables, est√°n en zonas de alta densidad.\n",
    "- **Todas las instancias excepto anomal√≠as** (puntos con etiqueta != -1).\n",
    "- **Todas las instancias** (incluso las que DBSCAN consider√≥ anomal√≠as). No suele ser recomendable.\n",
    "\n",
    "### Entrenamiento t√≠pico\n",
    "\n",
    "Es muy habitual **usar solo las instancias core** como datos de entrenamiento del clasificador:\n",
    "\n",
    "- Las instancias core son las que realmente \"definen\" los clusters.\n",
    "- Los puntos frontera o ruido no son tan fiables.\n",
    "\n",
    "---\n",
    "\n",
    "### Proceso de predicci√≥n de nuevas instancias\n",
    "\n",
    "El clasificador supervisado (por ejemplo KNN) que hemos entrenado s√≠ tiene `.predict()` y `.predict_proba()`, as√≠ que:\n",
    "\n",
    "- Le pasamos las nuevas instancias.\n",
    "- El clasificador devuelve la predicci√≥n de a qu√© cluster pertenece cada una.\n",
    "\n",
    "---\n",
    "\n",
    "### Refinamiento: detecci√≥n de anomal√≠as en las nuevas predicciones\n",
    "\n",
    "Una limitaci√≥n de este enfoque es que **el clasificador va a predecir s√≠ o s√≠ una clase para todos los puntos**.  \n",
    "Sin embargo, hay situaciones en las que querr√≠amos que ciertos puntos se considerasen **anomal√≠as**, como hace el propio DBSCAN.\n",
    "\n",
    "¬øC√≥mo podemos hacerlo?\n",
    "\n",
    "1. Al usar KNN (o cualquier clasificador basado en distancias), podemos obtener no solo la clase predicha, sino tambi√©n **la distancia al vecino m√°s cercano**.\n",
    "2. Podemos establecer un umbral (por ejemplo, la `eps` que hab√≠amos usado en DBSCAN) para decidir:\n",
    "    - Si la distancia al vecino m√°s cercano es **menor que el umbral**, aceptamos la predicci√≥n.\n",
    "    - Si la distancia es **mayor que el umbral**, consideramos el punto como anomal√≠a (`-1`).\n",
    "\n",
    "Este criterio nos permite **imitar el comportamiento de DBSCAN**, incluso cuando estamos prediciendo con un clasificador supervisado.\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen final\n",
    "\n",
    "- DBSCAN **no tiene `.predict()`** porque el algoritmo no se dise√±√≥ para nuevas predicciones fuera del conjunto de entrenamiento.\n",
    "- Para resolverlo, **entrenamos un clasificador supervisado** con los datos etiquetados por DBSCAN.\n",
    "- Es recomendable usar **solo las instancias core** como datos de entrenamiento.\n",
    "- Podemos usar la distancia al vecino m√°s cercano para establecer un criterio de **detecci√≥n de anomal√≠as** en nuevas predicciones.\n",
    "- DBSCAN es un algoritmo **potente y simple**, ideal para detectar clusters de cualquier forma y detectar outliers.\n",
    "- Limitaci√≥n: si la densidad var√≠a mucho entre clusters, o si el n√∫mero de instancias es muy grande, DBSCAN puede no ser la mejor opci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "### Ventajas e inconvenientes de DBSCAN\n",
    "\n",
    "| Ventajas | Inconvenientes |\n",
    "|----------|----------------|\n",
    "| No requiere especificar n¬∫ de clusters. | No permite `.predict()` directamente. |\n",
    "| Detecta clusters de cualquier forma. | Sensible a `eps` y `min_samples`. |\n",
    "| Detecta outliers autom√°ticamente. | Dificultad si la densidad de los clusters var√≠a mucho. |\n",
    "| Robusto a ruido. | Computaci√≥n no lineal con n¬∫ de instancias (lento con datasets grandes). |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef6dab",
   "metadata": {},
   "source": [
    "# Clustering Jer√°rquico: Aproximaci√≥n \"aglomerativa\" o bottom-up\n",
    "\n",
    "## Problema de negocio y motivaci√≥n\n",
    "\n",
    "El *Clustering Jer√°rquico* es una t√©cnica de agrupamiento que permite identificar patrones de similitud entre elementos de un conjunto de datos. Se usa cuando queremos explorar la estructura de los datos y generar grupos sin necesidad de especificar a priori el n√∫mero de grupos.\n",
    "\n",
    "**Ejemplo de aplicaci√≥n**: segmentaci√≥n de clientes en marketing, donde queremos identificar perfiles de cliente similares para adaptar campa√±as, promociones, etc.\n",
    "\n",
    "### Ventajas\n",
    "- Muy interpretable y visual.\n",
    "- Funciona bien con datasets peque√±os.\n",
    "- No requiere predefinir el n√∫mero de clusters (como K-Means).\n",
    "\n",
    "### Limitaciones\n",
    "- Escalabilidad (no adecuado para datasets muy grandes).\n",
    "- Los resultados no son directamente evaluables (problema no supervisado ‚Üí no hay etiquetas).\n",
    "\n",
    "---\n",
    "\n",
    "## Proceso general\n",
    "\n",
    "### 1. Obtenci√≥n y exploraci√≥n de los datos\n",
    "\n",
    "Aunque no se hace split de train/test en clustering jer√°rquico (no hay target), s√≠ se realiza un **mini EDA** con estos objetivos:\n",
    "\n",
    "- Verificar la necesidad de escalar o transformar variables num√©ricas.\n",
    "- Codificar correctamente las variables categ√≥ricas.\n",
    "- Explorar visualmente si ya existen agrupaciones claras que puedan orientar la selecci√≥n de features.\n",
    "\n",
    "### 2. Variables num√©ricas\n",
    "\n",
    "- Revisar escalas y distribuciones.\n",
    "- Evaluar necesidad de aplicar transformaciones (logaritmo, ra√≠z cuadrada, etc.).\n",
    "- Tener en cuenta que los algoritmos de clustering jer√°rquico se basan en distancias ‚Üí el escalado importa.\n",
    "\n",
    "### 3. Variables categ√≥ricas\n",
    "\n",
    "- Codificar adecuadamente (por ejemplo, binarizaci√≥n).\n",
    "- Revisar si es necesario escalar las num√©ricas para mantener la coherencia con las categ√≥ricas codificadas.\n",
    "- En caso de binaria (0/1), se puede plantear multiplicar por una constante para ajustar la escala.\n",
    "\n",
    "### 4. Selecci√≥n de features\n",
    "\n",
    "- Aunque se pueden usar todas las features, conviene evaluar combinaciones de 2 o 3 para visualizar potenciales clusters.\n",
    "- Esta selecci√≥n mejora la interpretabilidad y facilita comunicar resultados a stakeholders (por ejemplo, equipo de marketing).\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizaci√≥n: Dendrograma\n",
    "\n",
    "El **dendrograma** es la herramienta visual fundamental del clustering jer√°rquico.\n",
    "\n",
    "- Representa de forma gr√°fica c√≥mo se agrupan los elementos.\n",
    "- Permite decidir a posteriori cu√°ntos clusters utilizar (corte visual a cierta altura del dendrograma).\n",
    "\n",
    "### Claves para interpretar un dendrograma\n",
    "\n",
    "- El eje Y representa la distancia entre clusters en el espacio de las features seleccionadas.\n",
    "- Cortar el dendrograma a una altura concreta equivale a fijar un n√∫mero de clusters.\n",
    "- Los m√©todos de linkage determinan c√≥mo se calcula la \"distancia\" entre clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## Comprendiendo el Clustering Jer√°rquico\n",
    "\n",
    "### Tipos de Clustering Jer√°rquico\n",
    "\n",
    "- **Divisivo (top-down)**: parte de un √∫nico cluster y va dividi√©ndolo.\n",
    "- **Aglomerativo (bottom-up)**: parte de cada punto como su propio cluster y va fusion√°ndolos. Es el enfoque m√°s habitual.\n",
    "\n",
    "### Algoritmo de Clustering Jer√°rquico Aglomerativo (AHC)\n",
    "\n",
    "1. Se trata cada punto de datos como un cluster individual ‚Üí K clusters iniciales.\n",
    "2. Se unen los dos clusters m√°s cercanos ‚Üí K-1 clusters.\n",
    "3. Se repite el proceso hasta obtener un √∫nico cluster.\n",
    "4. El dendrograma refleja todo el proceso.\n",
    "\n",
    "### Importancia de la distancia entre clusters\n",
    "\n",
    "Se deben definir dos cosas:\n",
    "- **M√©trica de distancia** (por ejemplo, `euclidean`).\n",
    "- **M√©todo de linkage** (definici√≥n de \"distancia entre clusters\"):\n",
    "\n",
    "| M√©todo de linkage | Descripci√≥n |\n",
    "|-------------------|-------------|\n",
    "| single            | Distancia m√≠nima entre puntos de dos clusters |\n",
    "| complete          | Distancia m√°xima entre puntos de dos clusters |\n",
    "| average           | Promedio de distancias entre puntos de dos clusters |\n",
    "| ward              | Minimiza la varianza total dentro de los clusters (muy usado, asume distancia eucl√≠dea) |\n",
    "\n",
    "---\n",
    "\n",
    "## Funciones importantes\n",
    "\n",
    "### Dendrograma\n",
    "\n",
    "```python\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dendrogram(data, method='ward', metric='euclidean', figsize=(10,7)):\n",
    "    \"\"\"\n",
    "    Traza un dendrograma para visualizar la estructura jer√°rquica de los clusters.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Datos que se usar√°n para el clustering.\n",
    "        method (str): M√©todo de linkage ('single', 'complete', 'average', 'ward').\n",
    "        metric (str): M√©trica de distancia ('euclidean', 'manhattan', etc.).\n",
    "        figsize (tuple): Tama√±o de la figura.\n",
    "\n",
    "    Returns:\n",
    "        None. Muestra el dendrograma.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"Dendrograma\")\n",
    "    clusters = shc.linkage(data, method=method, metric=metric)\n",
    "    shc.dendrogram(Z=clusters)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeff633",
   "metadata": {},
   "source": [
    "## M√©todos de vinculaci√≥n (Linkage Methods)\n",
    "\n",
    "Hay muchos m√©todos de enlace. Al entender c√≥mo funcionan, podr√°s elegir el apropiado para tus necesidades. Cada uno dar√° diferentes resultados cuando se aplique. No hay una regla fija en el an√°lisis de agrupamiento: es recomendable **estudiar la naturaleza del problema**, probar diferentes m√©todos e inspeccionar los resultados.\n",
    "\n",
    "### Principales m√©todos de linkage\n",
    "\n",
    "#### V√≠nculo simple (Single Linkage)\n",
    "\n",
    "- Tambi√©n conocido como **vecino m√°s cercano** (*Nearest Neighbor*).\n",
    "- La distancia entre los cl√∫steres se define como la distancia entre sus miembros m√°s cercanos.\n",
    "\n",
    "**Ventajas**:\n",
    "- Tiende a alargar los clusters ‚Üí forma \"cadenas\".\n",
    "\n",
    "**Inconvenientes**:\n",
    "- Sensible al *ruido* y a outliers.\n",
    "\n",
    "#### Enlace completo (Complete Linkage)\n",
    "\n",
    "- Tambi√©n conocido como **vecino m√°s lejano** (*Furthest Neighbor*), *Voor Hees algorithm*.\n",
    "- La distancia entre los cl√∫steres se define como la distancia entre sus miembros m√°s lejanos.\n",
    "\n",
    "**Ventajas**:\n",
    "- Genera clusters compactos y esf√©ricos.\n",
    "\n",
    "**Inconvenientes**:\n",
    "- M√°s costoso computacionalmente.\n",
    "- Menos robusto si los clusters tienen formas irregulares.\n",
    "\n",
    "#### Enlace promedio (Average Linkage)\n",
    "\n",
    "- Tambi√©n conocido como **UPGMA** (*Unweighted Pair Group Method with Arithmetic Mean*).\n",
    "- Se calcula el promedio de distancias entre todos los pares de puntos posibles entre dos clusters.\n",
    "\n",
    "**Ventajas**:\n",
    "- Soluci√≥n intermedia entre Single y Complete linkage.\n",
    "- Produce resultados razonablemente equilibrados.\n",
    "\n",
    "#### Enlace del centroide (Centroid Linkage)\n",
    "\n",
    "- Tambi√©n conocido como **UPGMC** (*Unweighted Pair Group Method using Centroids*).\n",
    "- Se calcula el centroide de cada cluster (media de los puntos) y la distancia entre clusters se mide entre sus centroides.\n",
    "\n",
    "**Ventajas**:\n",
    "- Conceptualmente simple.\n",
    "- Tiende a funcionar bien en clusters con forma convexa.\n",
    "\n",
    "**Inconvenientes**:\n",
    "- Puede causar el fen√≥meno de **inversi√≥n** en el dendrograma (problema de inconsistencias).\n",
    "\n",
    "#### Enlace de Ward (Ward Linkage)\n",
    "\n",
    "- Tambi√©n conocido como **MISSQ** (*Minimal Increase of Sum-of-Squares*).\n",
    "- La distancia entre dos clusters se basa en el aumento del **Error de la Suma de Cuadrados (ESS)** que se producir√≠a si se fusionan.\n",
    "- Busca minimizar el aumento de ESS en cada paso.\n",
    "\n",
    "**Ventajas**:\n",
    "- Tiende a producir clusters muy homog√©neos.\n",
    "- Muy usado por defecto en la pr√°ctica.\n",
    "- Robusto.\n",
    "\n",
    "**Inconvenientes**:\n",
    "- Requiere utilizar una m√©trica de distancia adecuada, t√≠picamente **euclidiana**.\n",
    "\n",
    "---\n",
    "\n",
    "## Opciones de distancia\n",
    "\n",
    "Adem√°s del m√©todo de linkage, es necesario elegir una **m√©trica de distancia**. Algunas de las m√°s habituales:\n",
    "\n",
    "| M√©trica       | Descripci√≥n |\n",
    "|---------------|-------------|\n",
    "| Euclidiana    | Distancia geom√©trica \"recta\" entre puntos. Muy usada con Ward. |\n",
    "| Manhattan     | Suma de distancias absolutas (como moverse en un grid). |\n",
    "| Minkowski     | Generalizaci√≥n de Euclidiana y Manhattan. |\n",
    "| Chebyshev     | M√°xima diferencia en cualquiera de las dimensiones. |\n",
    "| Coseno        | Basada en el coseno del √°ngulo entre dos vectores (usada en texto). |\n",
    "| Jaccard       | Mide la similitud entre conjuntos (com√∫n en datos categ√≥ricos). |\n",
    "\n",
    "---\n",
    "\n",
    "## Recomendaci√≥n pr√°ctica\n",
    "\n",
    "- **M√©todo m√°s com√∫n** ‚Üí `Ward` con `Euclidiana`:\n",
    "  - Ward minimiza el error cuadr√°tico.\n",
    "  - Euclidiana es adecuada para espacios de baja dimensi√≥n.\n",
    "\n",
    "- Es la combinaci√≥n que hemos utilizado para el dendrograma del ejemplo.\n",
    "\n",
    "- En este caso concreto:\n",
    "  - 2 caracter√≠sticas (`Annual Income` y `Spending Score`).\n",
    "  - 200 observaciones.\n",
    "  - ‚Üí Espacio de baja dimensi√≥n (200 > 2), lo que hace que Euclidiana sea muy adecuada.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75641a5f",
   "metadata": {},
   "source": [
    "## Selecci√≥n de K a partir del dendrograma\n",
    "\n",
    "Ya hemos discutido m√©tricas, enlaces y c√≥mo cada uno puede impactar en nuestros resultados. Ahora continuamos con el an√°lisis del dendrograma para obtener una indicaci√≥n del n√∫mero de cl√∫steres.\n",
    "\n",
    "### M√©todo del mayor salto (gap)\n",
    "\n",
    "El dendrograma no nos da directamente un \"K √≥ptimo\". Sin embargo, podemos usar el **m√©todo del mayor salto (gap)**:\n",
    "\n",
    "- Consiste en buscar el espacio horizontal m√°s grande (el mayor gap) sin l√≠neas verticales.\n",
    "- Dicho de otra forma: identificar la distancia m√°s larga entre clusters antes de que se unan.\n",
    "- Se traza una l√≠nea horizontal a trav√©s de esa distancia.\n",
    "\n",
    "**Procedimiento**:\n",
    "- Trazar una l√≠nea horizontal en el dendrograma.\n",
    "- Contar cu√°ntas veces cruza l√≠neas verticales ‚Üí n√∫mero de clusters sugerido.\n",
    "\n",
    "### Interpretaci√≥n\n",
    "\n",
    "- Si al trazar una l√≠nea a altura `y=300` cruza 3 l√≠neas ‚Üí sugerencia de 3 clusters.\n",
    "- Si otra l√≠nea a `y=240` cruza 5 l√≠neas ‚Üí sugerencia de 5 clusters.\n",
    "- La decisi√≥n final se complementa con el an√°lisis de visualizaci√≥n de los datos (por ejemplo, scatter plot).\n",
    "\n",
    "**Nota**: el dendrograma es solo una **referencia**. Los resultados dependen del m√©todo de linkage y de la m√©trica utilizada. Se recomienda:\n",
    "\n",
    "- Probar diferentes combinaciones de enlace y m√©trica.\n",
    "- Comparar los resultados visualmente.\n",
    "- Utilizar el dendrograma como apoyo, no como √∫nica decisi√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementando un Clustering Jer√°rquico Aglomerativo\n",
    "\n",
    "Una vez decidimos el n√∫mero de clusters `K`, podemos aplicar el algoritmo con Scikit-Learn:\n",
    "\n",
    "### Ejemplo de implementaci√≥n\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering_model = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')\n",
    "clustering_model.fit(selected_data)\n",
    "data_labels = clustering_model.labels_\n",
    "```\n",
    "### Visualizaci√≥n del resultado\n",
    "```python\n",
    "sns.scatterplot(x='Annual Income (k$)', \n",
    "                y='Spending Score (1-100)', \n",
    "                data=selected_data, \n",
    "                hue=data_labels, palette=\"rainbow\").set_title('Labeled Customer Data')\n",
    "```\n",
    "\n",
    "### Interpretaci√≥n del resultado\n",
    "Ejemplo de interpretaci√≥n de clusters obtenidos:\n",
    "\n",
    "- Cluster 0: clientes con salarios altos pero bajos gastos ‚Üí clientes cautelosos.\n",
    "\n",
    "- Cluster 2: clientes con salarios altos y altos gastos ‚Üí clientes ideales para targeting.\n",
    "\n",
    "- Cluster 1: clientes con ingresos y gastos promedio ‚Üí gran volumen, interesante para campa√±as.\n",
    "\n",
    "- Cluster 4: clientes con salarios bajos y bajos gastos ‚Üí susceptibles a promociones.\n",
    "\n",
    "- Cluster 3: clientes con bajos ingresos pero altos gastos ‚Üí posiblemente a evitar.\n",
    "\n",
    "## Ventajas del agrupamiento jer√°rquico\n",
    "\n",
    "### Cu√°ndo usarlo\n",
    "\n",
    "- Cuando queremos **entender relaciones de similitud** entre observaciones.\n",
    "- Cuando nos interesa poder responder a preguntas como:\n",
    "    - \"*¬øQu√© observaciones son m√°s parecidas a esta en concreto?*\".\n",
    "- Cuando necesitamos una t√©cnica:\n",
    "    - Poco sensible a condiciones de inicializaci√≥n.\n",
    "    - Flexible para datos mixtos (num√©ricos + categ√≥ricos).\n",
    "    - Robusta frente a outliers.\n",
    "    - Capaz de captar relaciones jer√°rquicas.\n",
    "\n",
    "### Resumen de ventajas\n",
    "\n",
    "- **Nivel de detalle**:\n",
    "    - Proporciona informaci√≥n granular sobre qu√© observaciones son m√°s similares entre s√≠.\n",
    "    - No solo asigna una etiqueta de cluster, sino que permite explorar toda la jerarqu√≠a.\n",
    "- **Estabilidad**:\n",
    "    - No depende de condiciones de inicializaci√≥n (semillas aleatorias u orden del dataset).\n",
    "- **Adaptabilidad**:\n",
    "    - Puede adaptarse f√°cilmente para manejar datos mixtos (num√©ricos + categ√≥ricos) utilizando m√©tricas de distancia adecuadas.\n",
    "- **Familiaridad**:\n",
    "    - Despu√©s de K-Means, es uno de los m√©todos m√°s conocidos y estudiados, lo que facilita su aceptaci√≥n en entornos de negocio.\n",
    "- **Robustez frente a outliers**:\n",
    "    - Los outliers no afectan gravemente al proceso de clustering jer√°rquico aglomerativo.\n",
    "    - Tienden a unirse al final del proceso, cuando el resto de los datos ya est√°n agrupados.\n",
    "- **Flexibilidad en la forma de los clusters**:\n",
    "    - No hace suposiciones estrictas sobre la forma de los clusters.\n",
    "    - Dependiendo de la m√©trica de distancia y linkage utilizada, puede detectar clusters de formas variadas.\n",
    "\n",
    "---\n",
    "\n",
    "## Desventajas del agrupamiento jer√°rquico\n",
    "\n",
    "- **Escalabilidad limitada**:\n",
    "    - Es relativamente lento para datasets grandes.\n",
    "    - Requiere calcular la distancia entre **todas las observaciones**.\n",
    "    - La complejidad es aproximadamente O(n¬≤), por lo que no es adecuado para grandes vol√∫menes de datos.\n",
    "- **Sensibilidad a la m√©trica y m√©todo de linkage**:\n",
    "    - Los resultados pueden variar significativamente dependiendo de la combinaci√≥n elegida.\n",
    "    - Requiere experimentaci√≥n y validaci√≥n para garantizar resultados √∫tiles.\n",
    "- **No optimiza una funci√≥n global**:\n",
    "    - A diferencia de K-Means, no optimiza expl√≠citamente un objetivo claro (por ejemplo, minimizar la suma de cuadrados intra-cluster).\n",
    "    - La calidad de los clusters es m√°s dependiente del criterio de linkage que se utilice.\n",
    "- **No permite \"deshacer\" decisiones previas**:\n",
    "    - En el proceso aglomerativo, una vez que dos clusters se han fusionado, esa decisi√≥n no se puede revertir.\n",
    "    - Esto puede llevar a agrupaciones sub√≥ptimas si las primeras fusiones no fueron ideales.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159cd17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
